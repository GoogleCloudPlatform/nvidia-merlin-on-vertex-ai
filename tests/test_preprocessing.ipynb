{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Global variables and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'renatoleite-mldemos' # Change to your project Id.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "BUCKET =  'renatoleite-nvtabular' # Change to your bucket.\n",
    "\n",
    "IMAGE_VERSION = '21.11'\n",
    "IMAGE_NAME = 'nvt_preprocessing'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}:{IMAGE_VERSION}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = 'None'\n",
    "train_split = 'train'\n",
    "valid_split = 'valid'\n",
    "n_workers = 1\n",
    "recursive = False\n",
    "\n",
    "local_dev_folder = '/home/renatoleite/nvidia-merlin-on-vertex-ai/tests'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1/10 : FROM nvcr.io/nvidia/merlin/merlin-training:21.11\n",
      " ---> 8499f10347db\n",
      "Step 2/10 : WORKDIR /src\n",
      " ---> Using cache\n",
      " ---> b9308b55a8c8\n",
      "Step 3/10 : RUN pip install -U pip\n",
      " ---> Using cache\n",
      " ---> 7dd5a85a4cbf\n",
      "Step 4/10 : RUN pip install google-cloud-bigquery gcsfs\n",
      " ---> Using cache\n",
      " ---> 0f4d0d6917f9\n",
      "Step 5/10 : RUN pip install google-cloud-aiplatform==1.7.0 kfp==1.8.9\n",
      " ---> Using cache\n",
      " ---> 1d11165fdbc6\n",
      "Step 6/10 : COPY ./preprocessing ./preprocessing\n",
      " ---> Using cache\n",
      " ---> 488c732fc27a\n",
      "Step 7/10 : COPY ./serving ./serving\n",
      " ---> Using cache\n",
      " ---> fbe776a023e3\n",
      "Step 8/10 : COPY setup.py .\n",
      " ---> Using cache\n",
      " ---> abe269ebf12a\n",
      "Step 9/10 : COPY feature_utils.py .\n",
      " ---> Using cache\n",
      " ---> 9b11f7aae284\n",
      "Step 10/10 : RUN pip install -e .\n",
      " ---> Using cache\n",
      " ---> e82fb12c4cbc\n",
      "Successfully built e82fb12c4cbc\n",
      "Successfully tagged gcr.io/renatoleite-mldemos/nvt_preprocessing:21.11\n"
     ]
    }
   ],
   "source": [
    "! docker build -t $IMAGE_URI -f ../src/Dockerfile.nvtabular ../src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create local directory to read/write files\n",
    "\n",
    "This folder will work like a GCSfuse mount point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = f'/gcs/{BUCKET}'\n",
    "print(BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create BASE_DIR if not exists\n",
    "if not os.path.exists(BASE_DIR):\n",
    "    os.makedirs(BASE_DIR)\n",
    "    print(f'Directory \\\"{BASE_DIR}\\\" created')\n",
    "else:\n",
    "    print(f'Directory \\\"{BASE_DIR}\\\" already exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert CSV to Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training / Validation\n",
    "train_paths = 'gs://workshop-datasets/criteo/day_1' # Training CSV file to be preprocessed.\n",
    "valid_paths = 'gs://workshop-datasets/criteo/day_0' # Validation CSV file to be preprocessed.\n",
    "num_output_files_train = 1\n",
    "num_output_files_valid = 1\n",
    "convert_path = os.path.join(BASE_DIR, 'convert-csv-parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Training files from CSV to Parquet\n",
    "! docker run -it --rm --gpus all \\\n",
    "-v $local_dev_folder:/tests \\\n",
    "-v $BASE_DIR:$BASE_DIR \\\n",
    "$IMAGE_URI \\\n",
    "python /tests/test_preprocessing.py \\\n",
    "--method-to-call convert_csv_to_parquet_op \\\n",
    "--output-path $convert_path \\\n",
    "--data-paths $train_paths \\\n",
    "--split $train_split \\\n",
    "--num-output-files $num_output_files_train \\\n",
    "--n-workers $n_workers \\\n",
    "--recursive $recursive \\\n",
    "--device-limit-frac 0.8 \\\n",
    "--device-pool-frac 0.9 \\\n",
    "--part-mem-frac 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Validation files from CSV to Parquet\n",
    "! docker run -it --rm --gpus all \\\n",
    "-v $local_dev_folder:/tests \\\n",
    "-v $BASE_DIR:$BASE_DIR \\\n",
    "$IMAGE_URI \\\n",
    "python /tests/test_preprocessing.py \\\n",
    "--method-to-call convert_csv_to_parquet_op \\\n",
    "--output-path $convert_path \\\n",
    "--data-paths $valid_paths \\\n",
    "--split $valid_split \\\n",
    "--num-output-files $num_output_files_valid \\\n",
    "--n-workers $n_workers \\\n",
    "--recursive $recursive \\\n",
    "--device-limit-frac 0.8 \\\n",
    "--device-pool-frac 0.9 \\\n",
    "--part-mem-frac 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy converted files back to GCS\n",
    "! gsutil -m cp -r $convert_path gs://renatoleite-nvtabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit (analyse) dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = f'gs://{BUCKET}/convert-csv-parquet'\n",
    "workflow_path = f'/gcs/{BUCKET}/workflow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it --rm --gpus all \\\n",
    "-v $local_dev_folder:/tests \\\n",
    "-v $BASE_DIR:$BASE_DIR \\\n",
    "$IMAGE_URI \\\n",
    "python /tests/test_preprocessing.py \\\n",
    "--method-to-call analyze_dataset_op \\\n",
    "--output-path $parquet_path \\\n",
    "--workflow-path $workflow_path \\\n",
    "--split $train_split \\\n",
    "--n-workers $n_workers \\\n",
    "--recursive $recursive \\\n",
    "--device-limit-frac 0.8 \\\n",
    "--device-pool-frac 0.9 \\\n",
    "--part-mem-frac 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil -m cp -r $workflow_path gs://renatoleite-nvtabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_path = f'gs://{BUCKET}/convert-csv-parquet'\n",
    "workflow_path = f'/gcs/{BUCKET}/workflow'\n",
    "transformed_dataset = f'/gcs/{BUCKET}/transformed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it --rm --gpus all \\\n",
    "-v $local_dev_folder:/tests \\\n",
    "-v $BASE_DIR:$BASE_DIR \\\n",
    "$IMAGE_URI \\\n",
    "python /tests/test_preprocessing.py \\\n",
    "--method-to-call transform_dataset_op \\\n",
    "--output-path $parquet_path \\\n",
    "--workflow-path $workflow_path \\\n",
    "--transformed-dataset $transformed_dataset \\\n",
    "--split $train_split \\\n",
    "--n-workers $n_workers \\\n",
    "--recursive $recursive \\\n",
    "--device-limit-frac 0.8 \\\n",
    "--device-pool-frac 0.9 \\\n",
    "--part-mem-frac 0.125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! gsutil -m cp -r $transformed_dataset gs://renatoleite-nvtabular"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export Parquet files from BigQuery to GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f'gs://{BUCKET}/bq_export_parquet'\n",
    "bq_project = PROJECT_ID\n",
    "bq_location = 'us'\n",
    "bq_dataset_name = 'criteo_small'\n",
    "bq_table_name = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it --rm --gpus all \\\n",
    "-v $local_dev_folder:/tests \\\n",
    "-v $BASE_DIR:$BASE_DIR \\\n",
    "$IMAGE_URI \\\n",
    "python /tests/test_preprocessing.py \\\n",
    "--method-to-call export_parquet_from_bq_op \\\n",
    "--output-path $output_path \\\n",
    "--bq-project $bq_project \\\n",
    "--bq-location $bq_location \\\n",
    "--bq-dataset-name $bq_dataset_name \\\n",
    "--bq-table-name $bq_table_name \\\n",
    "--split $train_split"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
