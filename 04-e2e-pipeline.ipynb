{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7086876-72ed-4ac8-aeba-f05dbc641798",
   "metadata": {
    "id": "f7086876-72ed-4ac8-aeba-f05dbc641798"
   },
   "source": [
    "# End-to-end Recommender System with NVIDIA Merlin and Vertex AI.\n",
    "\n",
    "This notebook shows how to deploy and execute an end-to-end recommender system on Vertex Pipelines using NVIDIA Merlin.\n",
    "The notebook covers the following:\n",
    "\n",
    "1. Training pipeline overview.\n",
    "2. Set pipeline configurations.\n",
    "3. Build pipeline container images.\n",
    "4. Configure pipeline parameters.\n",
    "5. Compile KFP pipeline.\n",
    "6. Submit pipeline to Vertex AI.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13889502-7856-4d1f-bbf1-064a7978e225",
   "metadata": {
    "id": "13889502-7856-4d1f-bbf1-064a7978e225"
   },
   "source": [
    "## 1. Training Pipeline Overview\n",
    "\n",
    "The following diagram shows the end-to-end pipeline for preprocessing, training, and serving `NVIDIA Merlin` Recommender System using `Vertex AI`.\n",
    "The pipeline is defined in [src/training_pipelines.py](src/training_pipelines.py) module. \n",
    "\n",
    "The `training_bq` pipeline function reads the criteo data from `BigQuery` and perform the following steps:\n",
    "\n",
    "1. Preprocess the data using `NVTabular`, as described in the [01-dataset-preprocessing.ipynb](01-dataset-preprocessing.ipynb) notebook:\n",
    "    1. Export the `BigQuery` data to `Cloud Storage` as Parquet files.\n",
    "    2. Transform the data using an `NVTabular` workflow.\n",
    "    3. Write the transformed data as parquet files and the workflow object to `Cloud Storage`.\n",
    "2. Train a DeepFM model using `HugeCTR`. This step is submits a [Custom Training Job](https://cloud.google.com/vertex-ai/docs/training/create-custom-job) to `Vertex AI` training, as described in [02-model-training-hugectr.ipynb](02-model-training-hugectr.ipynb).\n",
    "3. Export the model as a `Triton` Ensemble to be served using `Triton` server. The ensemble consists of of the `NVTabular` preprocessing workflow and a `HugeCTR` model. \n",
    "4. The exported `Triton` ensemble model is uploaded to `Vertex AI` model resources.\n",
    "\n",
    "Once the model is uploaded to `Vertex AI`, a long with a reference to its serving `Triton` container, it can be deployed to `Vertex AI` Prediction, as described in [03-model-inference-hugectr.ipynb](03-model-inference-hugectr.ipynb). \n",
    "\n",
    "All the components of the pipelines are defined in the [src/pipelines/components.py](src/pipelines/components.py) module.\n",
    "\n",
    "<img src=\"images/merlin-vertex-e2e.png\" alt=\"Pipeline\" style=\"height: 50%; width:50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eee537-5fc7-480f-a011-ff56e8e623ab",
   "metadata": {
    "id": "54eee537-5fc7-480f-a011-ff56e8e623ab"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7e9c16-0c12-4ada-a71e-125b08b2b589",
   "metadata": {
    "id": "ec7e9c16-0c12-4ada-a71e-125b08b2b589"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa917976-d352-4f1b-aeb2-798f7cbbef83",
   "metadata": {
    "id": "fa917976-d352-4f1b-aeb2-798f7cbbef83"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'merlin-on-gcp' # Change to your project Id.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "BUCKET = 'merlin-on-gcp' # Change to your bucket.\n",
    "VERTEX_SA = f'vertex-sa@{PROJECT_ID}.iam.gserviceaccount.com'\n",
    "\n",
    "MODEL_NAME = 'deepfm'\n",
    "MODEL_VERSION = 'v01'\n",
    "MODEL_DISPLAY_NAME = f'criteo-hugectr-{MODEL_NAME}-{MODEL_VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "TRAINING_PIPELINE_NAME = f'merlin-training-pipeline'\n",
    "\n",
    "BQ_DATASET_NAME = 'criteo' # Set to your BigQuery dataset with the Criteo data.\n",
    "BQ_LOCATION = 'us' # Set to your BigQuery dataset location.\n",
    "BQ_TRAIN_TABLE_NAME = 'train'\n",
    "BQ_VALID_TABLE_NAME = 'valid'\n",
    "\n",
    "NVT_IMAGE_NAME = f'nvt-{MODEL_DISPLAY_NAME}'\n",
    "NVT_IMAGE_URI = f'gcr.io/{PROJECT_ID}/{NVT_IMAGE_NAME}'\n",
    "NVT_DOCKERNAME = 'nvtabular'\n",
    "\n",
    "HUGECTR_IMAGE_NAME = f'hugectr-{MODEL_DISPLAY_NAME}'\n",
    "HUGECTR_IMAGE_URI = f'gcr.io/{PROJECT_ID}/{HUGECTR_IMAGE_NAME}'\n",
    "HUGECTR_DOCKERNAME = 'hugectr'\n",
    "\n",
    "TRITON_IMAGE_NAME = f'triton-serving'\n",
    "TRITON_IMAGE_URI = f'gcr.io/{PROJECT_ID}/{HUGECTR_IMAGE_NAME}'\n",
    "TRITON_DOCKERNAME = 'triton'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9843d31-6716-40db-a88b-3eb2a051b2ea",
   "metadata": {
    "id": "d9843d31-6716-40db-a88b-3eb2a051b2ea"
   },
   "source": [
    "## 2. Set Pipeline Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f153b818-0d36-4101-a269-ada711ff76e6",
   "metadata": {
    "id": "f153b818-0d36-4101-a269-ada711ff76e6"
   },
   "outputs": [],
   "source": [
    "os.environ['PROJECT_ID'] = PROJECT_ID\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['WORKSPACE'] = WORKSPACE\n",
    "os.environ['VERTEX_SA'] = VERTEX_SA\n",
    "\n",
    "os.environ['BQ_DATASET_NAME'] = BQ_DATASET_NAME\n",
    "os.environ['BQ_LOCATION'] = BQ_LOCATION\n",
    "os.environ['BQ_TRAIN_TABLE_NAME'] = BQ_TRAIN_TABLE_NAME\n",
    "os.environ['BQ_VALID_TABLE_NAME'] = BQ_VALID_TABLE_NAME\n",
    "\n",
    "os.environ['TRAINING_PIPELINE_NAME'] = TRAINING_PIPELINE_NAME\n",
    "os.environ['MODEL_NAME'] = MODEL_NAME\n",
    "os.environ['MODEL_VERSION'] = MODEL_VERSION\n",
    "os.environ['MODEL_DISPLAY_NAME'] = MODEL_DISPLAY_NAME\n",
    "\n",
    "os.environ['MEMORY_LIMIT'] = '120G'\n",
    "os.environ['CPU_LIMIT'] = '32'\n",
    "os.environ['GPU_LIMIT'] = '4'\n",
    "os.environ['GPU_TYPE'] = 'nvidia-tesla-t4'\n",
    "\n",
    "os.environ['MACHINE_TYPE'] = 'a2-highgpu-4g'\n",
    "os.environ['ACCELERATOR_TYPE'] = 'NVIDIA_TESLA_A100'\n",
    "os.environ['ACCELERATOR_NUM'] = '1'\n",
    "os.environ['NUM_WORKERS'] = '3'\n",
    "\n",
    "os.environ['NUM_SLOTS'] = '26'\n",
    "os.environ['MAX_NNZ'] = '2'\n",
    "os.environ['EMBEDDING_VECTOR_SIZE'] = '11'\n",
    "os.environ['MAX_BATCH_SIZE'] = '64'\n",
    "os.environ['MODEL_REPOSITORY_PATH'] = '/models'\n",
    "\n",
    "os.environ['NVT_IMAGE_URI'] = NVT_IMAGE_URI\n",
    "os.environ['HUGECTR_IMAGE_URI'] = HUGECTR_IMAGE_URI\n",
    "os.environ['TRITON_IMAGE_URI'] = TRITON_IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcbe88a-30cc-478d-a040-5045f9aaabb9",
   "metadata": {
    "id": "fbcbe88a-30cc-478d-a040-5045f9aaabb9"
   },
   "source": [
    "The following cell lists the configuration values in `config.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502059d5-0218-43c7-b64f-1634bc884716",
   "metadata": {
    "id": "502059d5-0218-43c7-b64f-1634bc884716"
   },
   "outputs": [],
   "source": [
    "from src.pipelines import config\n",
    "import importlib\n",
    "importlib.reload(config)\n",
    "\n",
    "for key, value in config.__dict__.items():\n",
    "    if key.isupper(): print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7a468-618b-40e3-b0a5-546349801ff8",
   "metadata": {
    "id": "b5d7a468-618b-40e3-b0a5-546349801ff8"
   },
   "source": [
    "## 3. Build Pipeline Container Images\n",
    "\n",
    "The following three commands build the NVTabular preprocessing, HugeCTR training, and Triton serving container images using Cloud Build, and store the container images in Container Registry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f0ed33-51ea-4593-9a3c-0bba29dfbbd4",
   "metadata": {
    "id": "86f0ed33-51ea-4593-9a3c-0bba29dfbbd4"
   },
   "source": [
    "### Build NVTabular preprocessing container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead5075a-f081-4589-8eae-dae8d62edd0a",
   "metadata": {
    "id": "ead5075a-f081-4589-8eae-dae8d62edd0a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_LOCATION = './src'\n",
    "! gcloud builds submit --config src/cloudbuild.yaml --substitutions _DOCKERNAME=$NVT_DOCKERNAME,_IMAGE_URI=$NVT_IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION --timeout=2h --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2545dfd-0e4c-4113-8c3b-ccb6487596f5",
   "metadata": {
    "id": "c2545dfd-0e4c-4113-8c3b-ccb6487596f5"
   },
   "source": [
    "### Build HugeCTR training container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7565ad2c-f145-481c-ae09-c9055ba043b7",
   "metadata": {
    "id": "7565ad2c-f145-481c-ae09-c9055ba043b7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_LOCATION = './src'\n",
    "! gcloud builds submit --config src/cloudbuild.yaml --substitutions _DOCKERNAME=$HUGECTR_DOCKERNAME,_IMAGE_URI=$HUGECTR_IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION --timeout=2h --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966fbbb8-4f01-44d7-8877-3378c6a7af7c",
   "metadata": {
    "id": "966fbbb8-4f01-44d7-8877-3378c6a7af7c"
   },
   "source": [
    "### Build Triton serving container image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34c940b-77b1-40fd-b483-9f31dec06184",
   "metadata": {
    "id": "e34c940b-77b1-40fd-b483-9f31dec06184"
   },
   "outputs": [],
   "source": [
    "FILE_LOCATION = './src'\n",
    "! gcloud builds submit --config src/cloudbuild.yaml --substitutions _DOCKERNAME=$TRITON_DOCKERNAME,_IMAGE_URI=$TRITON_IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION --timeout=2h --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee19e322-1ddf-4127-a31e-6992a582c797",
   "metadata": {
    "id": "ee19e322-1ddf-4127-a31e-6992a582c797",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## 4. Configure pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3fb0f-d012-4f31-81da-e10bf99d0647",
   "metadata": {
    "id": "50a3fb0f-d012-4f31-81da-e10bf99d0647"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 0\n",
    "MAX_ITERATIONS = 50000\n",
    "EVAL_INTERVAL = 1000\n",
    "EVAL_BATCHES = 500\n",
    "EVAL_BATCHES_FINAL = 2500\n",
    "DISPLAY_INTERVAL = 200\n",
    "SNAPSHOT_INTERVAL = 0\n",
    "PER_GPU_BATCHSIZE = 2048\n",
    "LR = 0.001\n",
    "DROPOUT_RATE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c282810b-c760-4de7-a00e-8cf72c33f829",
   "metadata": {
    "id": "c282810b-c760-4de7-a00e-8cf72c33f829"
   },
   "outputs": [],
   "source": [
    "parameter_values = {\n",
    "    'shuffle': json.dumps(None), # select PER_PARTITION, PER_WORKER, FULL, or None.\n",
    "    'per_gpu_batch_size': PER_GPU_BATCHSIZE,\n",
    "    'max_iter': MAX_ITERATIONS,\n",
    "    'max_eval_batches': EVAL_BATCHES ,\n",
    "    'eval_batches': EVAL_BATCHES_FINAL ,\n",
    "    'dropout_rate': DROPOUT_RATE,\n",
    "    'lr': LR ,\n",
    "    'num_epochs': NUM_EPOCHS,\n",
    "    'eval_interval': EVAL_INTERVAL,\n",
    "    'snapshot': SNAPSHOT_INTERVAL,\n",
    "    'display_interval': DISPLAY_INTERVAL\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93628fa4-41e0-4ac2-9162-8c971383fb74",
   "metadata": {
    "id": "93628fa4-41e0-4ac2-9162-8c971383fb74"
   },
   "source": [
    "## 5. Compile KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc54b32-c48b-4d66-882f-6bd0e29678e1",
   "metadata": {
    "id": "0dc54b32-c48b-4d66-882f-6bd0e29678e1"
   },
   "outputs": [],
   "source": [
    "from src.pipelines import training_pipelines\n",
    "\n",
    "compiled_pipeline_path = 'merlin_training_bq.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=training_pipelines.training_bq,\n",
    "       package_path=compiled_pipeline_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e4e255-f4a6-474c-aed4-d225cefdfbd5",
   "metadata": {
    "id": "41e4e255-f4a6-474c-aed4-d225cefdfbd5"
   },
   "source": [
    "## 6. Submit pipeline to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ecf7a1-9b79-4386-baa3-e5ff74347ac0",
   "metadata": {
    "id": "f8ecf7a1-9b79-4386-baa3-e5ff74347ac0"
   },
   "outputs": [],
   "source": [
    "job_name = f'merlin_training_bq_{datetime.now().strftime(\"%Y%m%d%H%M%S\")}'\n",
    "\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    display_name=job_name,\n",
    "    template_path=compiled_pipeline_path,\n",
    "    enable_caching=False,\n",
    "    parameter_values=parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3534ea-3cce-4a73-8b8e-594e21d6403a",
   "metadata": {
    "id": "7e3534ea-3cce-4a73-8b8e-594e21d6403a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "04-e2e-pipeline.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m87",
   "type": "gcloud",
   "uri": "gcr.io/merlin-on-gcp/merlin-vertex-dev@sha256:5af46d488ff0ff373c348d87010c0e3ca22fa075db37c57e7c12a473e5954e4f"
  },
  "kernelspec": {
   "display_name": "Custom [merlin-vertex-dev] (Local)",
   "language": "python",
   "name": "local-gcr.io_merlin-on-gcp_merlin-vertex-dev_sha256_5af46d488ff0ff373c348d87010c0e3ca22fa075db37c57e7c12a473e5954e4f__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
