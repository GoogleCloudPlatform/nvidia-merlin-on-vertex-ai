{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preprocessing at Scale with NVIDIA Merlin NVTabular and Vertex AI\n",
    "\n",
    "This notebook demonstrates how to preprocess data using [NVIDIA Merlin NVTabular](https://developer.nvidia.com/nvidia-merlin/nvtabular) and [Vertex AI](https://cloud.google.com/vertex-ai). The notebook covers the following:  \n",
    "1. NVTabular Overview.  \n",
    "2. Preprocessing Criteo Dataset.  \n",
    "3. Preprocessing Pipeline on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Merlin NVTabular Overview\n",
    "\n",
    "Merlin NVTabular is a feature engineering and preprocessing library designed to effectively manipulate \n",
    "large datasets and significantly reduce data preparation time. The [core features](https://github.com/NVIDIA-Merlin/NVTabular/blob/main/docs/source/core_features.md) of NVTabular include:\n",
    "\n",
    "* Processes large datasets not bound by CPU or GPU memory.\n",
    "* Accelerates data preprocessing computation on NVIDIA GPUs using the [RAPIDS cuDF](https://github.com/rapidsai/cudf/tree/main/python/dask_cudf) library.\n",
    "* Supports multi-GPU and multi-node scaling with [DASK-CUDA](https://github.com/rapidsai/dask-cuda) and [dask.distributed](https://distributed.dask.org/en/latest/) parallelism.\n",
    "* Supports tabular data formats, including comma-separated values (CSV) files, Apache Parquet, Apache Orc, and Apache Avro.\n",
    "* Provides data loaders that are optimized for TensorFlow, PyTorch, and Merlin HugeCTR.\n",
    "* Includes multi-hot categoricals and vector continuous passing support to ease feature engineering.\n",
    "\n",
    "\n",
    "To preprocess the data, we need to define a transformation [`Workflow`](https://nvidia-merlin.github.io/NVTabular/main/api/workflow/workflow.html).  \n",
    "Each transformation step in the transformation pipeline executes multiple calculations, called `ops`. \n",
    "NVTabular provides a [set of ops](c), which include:\n",
    "\n",
    " - Filtering outliers or missing values, or creating new features indicating that a value is missing;\n",
    " - Imputing and filling in missing data;\n",
    " - Discretization or bucketing of continuous features;\n",
    " - Creating features by splitting or combining existing features, for example, breaking down a date column into day-of-week, month-of-year, day-of-month features;\n",
    " - Normalizing numerical features to have zero mean and unit variance or applying transformations, for example with log transform;\n",
    " - Encoding discrete features using one-hot vectors or converting them to continuous integer indices.  \n",
    "\n",
    "NVTabular processes a dataset, given a pre-defined workflow, in two steps:\n",
    "\n",
    "1. The `fit` step, where NVTabular compute the statistics required for transforming the data. Such a step requires at most `N` passes through the data, where `N` is the number of chained operations in the workflow.\n",
    "2. The `apply` step, where NVTabular uses the fitted workflow to process the data. \n",
    "\n",
    "NVTabular is designed to minimize the number of passes through the data. This is achieved with a lazy execution strategy. Data operations are not executed until an explicit apply phase. This allows NVTabular to optimize the workflow that requires iteration over the entire dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Preprocessing Criteo dataset\n",
    "\n",
    "The Criteo dataset contains over four billion samples spanning 24 CSV files. Each record contains 40 columns: 13 columns are numerical, 26 columns are categorical, and 1 binary target column.  \n",
    "See [00-dataset-management.ipynb](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/00-dataset-management.ipynb) for more details.\n",
    "\n",
    "\n",
    "### NVTabular preprocessing Workflow for Criteo dataset\n",
    "\n",
    "In this example, the preprocessing `nvt.Workflow` consists for the following operations:\n",
    " - [Categorify](https://nvidia-merlin.github.io/NVTabular/main/api/ops/categorify.html): applied to categorical columns (column names that start with C). \n",
    " - [FillMissing](https://nvidia-merlin.github.io/NVTabular/main/api/ops/fillmissing.html): applied to continuous columns (column names that start with I).\n",
    " - [Clip](https://nvidia-merlin.github.io/NVTabular/main/api/ops/clip.html):  applied to continuous columns after FillMissing.\n",
    " - [Normalize](https://nvidia-merlin.github.io/NVTabular/main/api/ops/normalize.html): applied to continuous columns after Clip.\n",
    " \n",
    "<img src=\"images/dag_preprocessing.png\" alt=\"Pipeline\" style=\"width:30%;\"/>\n",
    " \n",
    " The `nvt.Workflow` is created in the `create_criteo_nvt_workflow` method, which can be found in [src/preprocessing/task.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/preprocessing/task.py) module.  \n",
    " This `nvt.Workflow` will be used as a guide to calculate the necessary statistics, and execute the data transformation.  \n",
    " \n",
    "\n",
    "### Implementing the preprocessing pipelines using KFP\n",
    "\n",
    "[src/pipelines/preprocessing_pipelines.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/preprocessing_pipelines.py) defines the KFP pipelines to preprocess the Criteo data. \n",
    "The `preprocessing_csv` processes the CSV data files in Cloud Storage.\n",
    "\n",
    "A pipeline component is a self-contained set of code that performs one step in your ML workflow. The pipeline uses the following components defined in [src/pipelines/components.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/components.py):\n",
    "\n",
    "1. `convert_csv_to_parquet_op`: this component converts raw CSV files to Parquet files, and store them to Cloud Storage. \n",
    "2. `analyze_dataset_op`: this component creates a Criteo preprocessing `nvt.Workflow`, fit it to the training data split, and store it to Cloud Storage.\n",
    "3. `transform_dataset_op`: this component loads the fitted `nvt.Workflow` from Cloud Storage, uses it to transform and input datas split, and store the transformed data as Parquet files to Cloud Storage.\n",
    "\n",
    "Each component is annotated with Inputs and Outputs to keep track of lineage metadata.  \n",
    "The docker image used to execute the components is defined in [Dockerfile.nvtabular](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/Dockerfile.nvtabular).  \n",
    "\n",
    "Some steps in the pipeline are configured to submit a custom Vertex AI Training job with the required CPU, memory and GPU configurations.  \n",
    "You can customize the pipeline by setting the variables in the [config.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/config.py) module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this section of the notebook you configure your environment settings, including a GCP project, a GCP compute region, a Vertex AI service account and a Vertex AI staging bucket. You also set the locations of training and validation splits in GCS.\n",
    "\n",
    "Make sure to update the below cells with the values reflecting your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project definitions\n",
    "PROJECT_ID = 'renatoleite-dev' # Change to your project ID.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "\n",
    "# Bucket definitions\n",
    "BUCKET =  'merlin-on-gcp' # Change to your bucket. All the files will be stored here.\n",
    "VERSION = 'v03'\n",
    "MODEL_DISPLAY_NAME = f'criteo-merlin-recommender-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# Docker definitions\n",
    "IMAGE_NAME = 'nvt_preprocessing'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "DOCKERNAME = 'nvtabular'\n",
    "\n",
    "# Pipeline definitions\n",
    "PREPROCESS_CSV_PIPELINE_NAME = 'nvt-csv-pipeline'\n",
    "PREPROCESS_CSV_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_CSV_PIPELINE_NAME)\n",
    "\n",
    "# Instance configuration\n",
    "# Change if you need a different instance configuration\n",
    "INSTANCE_TYPE = 'n1-highmem-64'\n",
    "GPU_LIMIT = '4'\n",
    "GPU_TYPE = 'NVIDIA_TESLA_T4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pipeline configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROJECT_ID'] = PROJECT_ID\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['WORKSPACE'] = WORKSPACE\n",
    "\n",
    "os.environ['NVT_IMAGE_URI'] = IMAGE_URI\n",
    "os.environ['PREPROCESS_CSV_PIPELINE_NAME'] = PREPROCESS_CSV_PIPELINE_NAME\n",
    "os.environ['PREPROCESS_CSV_PIPELINE_ROOT'] = PREPROCESS_CSV_PIPELINE_ROOT\n",
    "os.environ['DOCKERNAME'] = DOCKERNAME\n",
    "\n",
    "os.environ['INSTANCE_TYPE'] = INSTANCE_TYPE\n",
    "os.environ['GPU_LIMIT'] = GPU_LIMIT\n",
    "os.environ['GPU_TYPE'] = GPU_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex SDK client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI API\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=os.path.join(WORKSPACE, 'stg') \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Container Docker Image\n",
    "\n",
    "The following command will build the Docker container image to the NVTabular preprocessing steps of the pipeline and push it to the [Google Container Registry](https://cloud.google.com/container-registry). \n",
    "\n",
    "Note that building the Docker container image take up to 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_LOCATION = './src'\n",
    "! gcloud builds submit --config src/cloudbuild.yaml --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION --timeout=2h --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. CSV Preprocessing Pipeline Execution\n",
    "\n",
    "The CSV Criteo data preprocessing pipeline performs the following steps.  \n",
    "\n",
    " 1. Read CSV files from Cloud Storage.\n",
    " 2. Convert the CSV files to parquet format and write it Cloud Storage.\n",
    " 3. Fit a pre-defined NVTabular workflow to the training data split to calculate transformation statistics.\n",
    " 4. Transform the training and validation data splits using the fitted workflow.\n",
    " 5. Output transformed parquet files to Cloud Storage.\n",
    "\n",
    "<img src=\"./images/preprocessing_pipeline_csv.png\" alt=\"Pipeline\" style=\"height: 50%; width:50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting CSV files to Parquet with NVTabular\n",
    "\n",
    "The Criteo dataset is provided in TSV format, but the recommended data format to run the NVTabular preprocessing task and get the best possible performance is [Parquet](http://parquet.apache.org/documentation/latest/); a compressed, column-oriented file structure format. While NVTabular also supports reading from CSV files, reading  \n",
    "Parquet files can be 2x faster than reading CSV files.  \n",
    "\n",
    "To convert the Criteo CSV data to Parquet, the following steps are performed:\n",
    "\n",
    "1. Create a `nvt.Dataset` object the CSV data using the `create_csv_dataset` method in [src/preprocessing/task.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/preprocessing/task.py).\n",
    "2. Convert the CSV data to Parquet, and write it to Cloud Storahe using the `convert_csv_to_parquet` method in [src/preprocessing/task.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/preprocessing/task.py).\n",
    "\n",
    "The pipeline uses the `convert_csv_to_parquet_op` component, which is implemented in [src/pipelines/components.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/components.py) which submits a Vertex AI training job to convert the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline parameters\n",
    "\n",
    "NVTabular provides an option to shuffle the dataset before storing to disk.  \n",
    "The uniformly shuffled dataset enables the data loader to read in contiguous chunks of data that are already randomized across the entire dataset.\n",
    "NVTabular provides the option to control the number of chunks that are combined into a batch, allowing the end user flexibility when trading off between performance and true randomization.  \n",
    "This mechanism is critical when dealing with datasets that exceed CPU memory and per-epoch shuffling is desired during training.  \n",
    "Full shuffling of such a dataset can exceed training time for the epoch by several orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of path(s) to criteo file(s) or folder(s) in GCS.\n",
    "# Training files\n",
    "TRAIN_PATHS = ['gs://renatoleite-criteo-full'] \n",
    "# Validation files\n",
    "VALID_PATHS = ['gs://renatoleite-criteo-full/day_0'] \n",
    "\n",
    "sep = '\\t' # Separator for the CSV file.\n",
    "num_output_files_train = 24 # Number of output files after converting CSV to Parquet\n",
    "num_output_files_valid = 1 # Number of output files after converting CSV to Parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_parameter_values = {\n",
    "    'train_paths': json.dumps(TRAIN_PATHS),\n",
    "    'valid_paths': json.dumps(VALID_PATHS),\n",
    "    'sep': sep,\n",
    "    'num_output_files_train': num_output_files_train,\n",
    "    'num_output_files_valid': num_output_files_valid,\n",
    "    'shuffle': json.dumps(None) # select PER_PARTITION, PER_WORKER, FULL, or None.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.preprocessing_pipelines import preprocessing_csv\n",
    "\n",
    "csv_compiled_pipeline_path = f'{PREPROCESS_CSV_PIPELINE_NAME}.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_csv,\n",
    "       package_path=csv_compiled_pipeline_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'{datetime.now().strftime(\"%Y%m%d%H%M%S\")}_{PREPROCESS_CSV_PIPELINE_NAME}'\n",
    "\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    display_name=job_name,\n",
    "    template_path=csv_compiled_pipeline_path,\n",
    "    enable_caching=False,\n",
    "    parameter_values=csv_parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "After completing this notebook you can proceed to the [02-model-training-hugectr.ipynb](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/02-model-training-hugectr.ipynb) notebook that demonstrates how to train DeepFM model using NVIDIA HugeCTR and Vertex AI."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "managed-notebooks.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/managed-notebooks:m87"
  },
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
