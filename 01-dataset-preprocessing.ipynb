{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preprocessing at Scale with NVIDIA Merlin NVTabular and Vertex AI\n",
    "\n",
    "This notebook demonstrates how to preprocess data using [NVIDIA Merlin NVTabular](https://developer.nvidia.com/nvidia-merlin/nvtabular) and [Vertex AI](https://cloud.google.com/vertex-ai). The notebook covers the following:  \n",
    "1. NVTabular Overview.  \n",
    "2. Preprocessing Criteo Dataset.  \n",
    "3. Preprocessing Pipeline on Vertex AI - two options:\n",
    "    3.1. CSV preprocessing pipeline execution.  \n",
    "    3.2. BigQuery preprocessing pipeline execution.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Merlin NVTabular Overview\n",
    "\n",
    "Merlin NVTabular is a feature engineering and preprocessing library designed to effectively manipulate \n",
    "large datasets and significantly reduce data preparation time. The [core features](https://github.com/NVIDIA-Merlin/NVTabular/blob/main/docs/source/core_features.md) of NVTabular include:\n",
    "\n",
    "* Processes large datasets not bound by CPU or GPU memory.\n",
    "* Accelerates data preprocessing computation on NVIDIA GPUs using the [RAPIDS cuDF](https://github.com/rapidsai/cudf/tree/main/python/dask_cudf) library.\n",
    "* Supports multi-GPU and multi-node scaling with [DASK-CUDA](https://github.com/rapidsai/dask-cuda) and [dask.distributed](https://distributed.dask.org/en/latest/) parallelism.\n",
    "* Supports tabular data formats, including comma-separated values (CSV) files, Apache Parquet, Apache Orc, and Apache Avro.\n",
    "* Provides data loaders that are optimized for TensorFlow, PyTorch, and Merlin HugeCTR.\n",
    "* Includes multi-hot categoricals and vector continuous passing support to ease feature engineering.\n",
    "\n",
    "\n",
    "To preprocess the data, we need to define a transformation [`Workflow`](https://nvidia-merlin.github.io/NVTabular/main/api/workflow/workflow.html).  \n",
    "Each transformation step in the transformation pipeline executes multiple calculations, called `ops`. \n",
    "NVTabular provides a [set of ops](c), which include:\n",
    "\n",
    " - Filtering outliers or missing values, or creating new features indicating that a value is missing;\n",
    " - Imputing and filling in missing data;\n",
    " - Discretization or bucketing of continuous features;\n",
    " - Creating features by splitting or combining existing features, for example, breaking down a date column into day-of-week, month-of-year, day-of-month features;\n",
    " - Normalizing numerical features to have zero mean and unit variance or applying transformations, for example with log transform;\n",
    " - Encoding discrete features using one-hot vectors or converting them to continuous integer indices.  \n",
    "\n",
    "NVTabular processes a dataset, given a pre-defined workflow, in two steps:\n",
    "\n",
    "1. The `fit` step, where NVTabular compute the statistics required for transforming the data. Such a step requires at most `N` passes through the data, where `N` is the number of chained operations in the workflow.\n",
    "2. The `apply` step, where NVTabular uses the fitted workflow to process the data. \n",
    "\n",
    "NVTabular is designed to minimize the number of passes through the data. This is achieved with a lazy execution strategy. Data operations are not executed until an explicit apply phase. This allows NVTabular to optimize the workflow that requires iteration over the entire dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Preprocessing Criteo dataset\n",
    "\n",
    "The Criteo dataset contains over four billion samples spanning 24 CSV files. Each record contains 40 columns: 13 columns are numerical, 26 columns are categorical, and 1 binary target column. See [00-dataset-management.ipynb](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/00-dataset-management.ipynb) for more details.\n",
    "\n",
    "\n",
    "### NVTabular preprocessing Workflow for Criteo dataset\n",
    "\n",
    "In this example, the preprocessing `nvt.Workflow` consists for the following operations:\n",
    " - [Categorify](https://nvidia-merlin.github.io/NVTabular/main/api/ops/categorify.html): applied to categorical columns (column names that start with C). \n",
    " - [FillMissing](https://nvidia-merlin.github.io/NVTabular/main/api/ops/fillmissing.html): applied to continuous columns (column names that start with I).\n",
    " - [Clip](https://nvidia-merlin.github.io/NVTabular/main/api/ops/clip.html):  applied to continuous columns after FillMissing.\n",
    " - [Normalize](https://nvidia-merlin.github.io/NVTabular/main/api/ops/normalize.html): applied to continuous columns after Clip.\n",
    " \n",
    "<img src=\"images/dag_preprocessing.png\" alt=\"Pipeline\" style=\"height: 50%; width:50%;\"/>\n",
    " \n",
    " The `nvt.Workflow` is created in the `create_criteo_nvt_workflow` method, which can be found in [src/preprocessing/etl.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/preprocessing/etl.py) module.  \n",
    " This `nvt.Workflow` will be used as a guide to calculate the necessary statistics, and execute the data transformation.  \n",
    " \n",
    "\n",
    "### Implementing the preprocessing pipelines using KFP\n",
    "\n",
    "[src/pipelines/preprocessing_pipelines.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/preprocessing_pipelines.py) defines the KFP pipelines to preprocess the Criteo data. \n",
    "The `preprocessing_csv` processes the CSV data files in Cloud Storage, while `preprocessing_bq` processes the data stored in BigQuery.\n",
    "\n",
    "A pipeline component is a self-contained set of code that performs one step in your ML workflow. The pipeline uses the following components defined in [src/pipelines/components.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/components.py):\n",
    "\n",
    "1. `convert_csv_to_parquet_op`: this component converts raw CSV files to Parquet files, and store them to Cloud Storage. \n",
    "2. `analyze_dataset_op`: this component creates a Criteo preprocessing `nvt.Workflow`, fit it to the training data split, and store it to Cloud Storage.\n",
    "3. `transform_dataset_op`: this component loads the fitted `nvt.Workflow` from Cloud Storage, uses it to transform and input datas split, and store the transformed data as Parquet files to Cloud Storage.\n",
    "\n",
    "Each component is annotated with Inputs and Outputs to keep track of lineage metadata.\n",
    "The `base_image` used to execute the components is defined in [Dockerfile.nvtabular](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/Dockerfile.nvtabular). \n",
    "\n",
    "Each step in the pipeline is configured with the required CPU, memory and GPU configurations, as follows:\n",
    "\n",
    "```\n",
    "component_being_executed.set_cpu_limit(\"8\") # Number of CPUs\n",
    "component_being_executed.set_memory_limit(\"32G\") # Memory quantity\n",
    "component_being_executed.set_gpu_limit(\"1\") # Number of GPUs\n",
    "component_being_executed.add_node_selector_constraint('cloud.google.com/gke-accelerator', 'nvidia-tesla-t4') # GPU type\n",
    "```\n",
    "\n",
    "See [Specify machine type for a pipeline step](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline#specify-machine-type) for more information.\n",
    "\n",
    "\n",
    "You can configure the pipeline by setting the variables in the [config.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/config.py) module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this section of the notebook you configure your environment settings, including a GCP project, a GCP compute region, a Vertex AI service account and a Vertex AI staging bucket. You also set the locations of training and validation splits in GCS.\n",
    "\n",
    "Make sure to update the below cells with the values reflecting your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = 'renatoleite-mldemos' # Change to your project Id.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "DATASET_GCS_LOCATION = 'gs://workshop-datasets/criteo'\n",
    "BUCKET =  'merlin-on-gcp' # Change to your bucket.\n",
    "\n",
    "VERSION = 'v01'\n",
    "MODEL_DISPLAY_NAME = f'criteo-merlin-recommender-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "IMAGE_NAME = 'nvt_preprocessing'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "DOCKERNAME = 'nvtabular'\n",
    "\n",
    "PREPROCESS_CSV_PIPELINE_NAME = 'nvt-csv-pipeline'\n",
    "PREPROCESS_CSV_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_CSV_PIPELINE_NAME)\n",
    "\n",
    "PREPROCESS_BQ_PIPELINE_NAME =  'nvt-bq-pipeline'\n",
    "PREPROCESS_BQ_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_BQ_PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pipeline configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROJECT_ID'] = PROJECT_ID\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['WORKSPACE'] = WORKSPACE\n",
    "\n",
    "os.environ['NVT_IMAGE_URI'] = IMAGE_URI\n",
    "os.environ['PREPROCESS_CSV_PIPELINE_NAME'] = PREPROCESS_CSV_PIPELINE_NAME\n",
    "os.environ['PREPROCESS_CSV_PIPELINE_ROOT'] = PREPROCESS_CSV_PIPELINE_ROOT\n",
    "os.environ['DOCKERNAME'] = DOCKERNAME\n",
    "\n",
    "os.environ['MEMORY_LIMIT'] = '120G'\n",
    "os.environ['CPU_LIMIT'] = '32'\n",
    "os.environ['GPU_LIMIT'] = '4'\n",
    "os.environ['GPU_TYPE'] = 'nvidia-tesla-t4'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex SDK client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=os.path.join(WORKSPACE, 'stg') \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Container Docker Image\n",
    "\n",
    "The following command will build the Docker container image to the NVTabular preprocessing steps of the pipeline and push it to the [Google Container Registry](https://cloud.google.com/container-registry). \n",
    "\n",
    "Note that building the Docker container image takes around 8 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_LOCATION = './src'\n",
    "! gcloud builds submit --config src/cloudbuild.yaml --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. CSV Preprocessing Pipeline Execution\n",
    "\n",
    "The CSV Criteo data preprocessing pipeline performs the following steps.  \n",
    "\n",
    " 1. Read CSV files from Cloud Storage.\n",
    " 2. Convert the CSV files to parquet format and write it Cloud Storage.\n",
    " 3. Fit a pre-defined NVTabular workflow to the training data split to calculate transformation statistics.\n",
    " 4. Transform the training and validation data splits using the fitted workflow.\n",
    " 5. Output transformed parquet files to Cloud Storage.\n",
    "\n",
    "\n",
    "<img src=\"./images/preprocessing_pipeline_csv.png\" alt=\"Pipeline\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting CSV files to Parquet with NVTabular\n",
    "\n",
    "The Criteo dataset is provided in CSV format, but the recommended data format to run the NVTabular preprocessing task and get the best possible performance is [Parquet](http://parquet.apache.org/documentation/latest/); a compressed, column-oriented file structure format. While NVTabular also supports reading from CSV files, reading  \n",
    "Parquet files can 2X faster than reading CSV files.  \n",
    "\n",
    "To convert the Criteo CSV data to Parquet, the following steps are performed:\n",
    "\n",
    "1. Create a `nvt.Dataset` object the CSV data using the `create_csv_dataset` method in [src/preprocessing/etl.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/preprocessing/etl.py).\n",
    "2. Convert the CSV data to Parquet, and write it to Cloud Storahe using the `convert_csv_to_parquet` method in [src/preprocessing/etl.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/preprocessing/etl.py).\n",
    "\n",
    "The pipeline uses the `convert_csv_to_parquet_op` component, which is implemented in [src/pipelines/components.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/components.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline parameters\n",
    "\n",
    "In NVTabular, NVIDIA provides an option to shuffle the dataset before storing to disk.  \n",
    "The uniformly shuffled dataset enables the data loader to read in contiguous chunks of data that are already randomized across the entire dataset.\n",
    "NVTabular provides the option to control the number of chunks that are combined into a batch, allowing the end user flexibility when trading off between performance and true randomization.  \n",
    "This mechanism is critical when dealing with datasets that exceed CPU memory and per-epoch shuffling is desired during training.  \n",
    "Full shuffling of such a dataset can exceed training time for the epoch by several orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATHS = ['gs://workshop-datasets/criteo/day_1'] # Training CSV file to be preprocessed.\n",
    "VALID_PATHS = ['gs://workshop-datasets/criteo/day_0'] # Validation CSV file to be preprocessed.\n",
    "sep = '\\t' # Separator for the CSV file.\n",
    "num_output_files_train = 4\n",
    "num_output_files_valid = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_parameter_values = {\n",
    "    'train_paths': json.dumps(TRAIN_PATHS),\n",
    "    'valid_paths': json.dumps(VALID_PATHS),\n",
    "    'sep': sep,\n",
    "    'shuffle': json.dumps(None), # select PER_PARTITION, PER_WORKER, FULL, or None.\n",
    "    'num_output_files_train': num_output_files_train,\n",
    "    'num_output_files_valid': num_output_files_valid\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.preprocessing_pipelines import preprocessing_csv\n",
    "\n",
    "csv_compiled_pipeline_path = f'{PREPROCESS_CSV_PIPELINE_NAME}.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_csv,\n",
    "       package_path=csv_compiled_pipeline_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'{datetime.now().strftime(\"%Y%m%d%H%M%S\")}_{PREPROCESS_CSV_PIPELINE_NAME}'\n",
    "\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    display_name=job_name,\n",
    "    template_path=csv_compiled_pipeline_path,\n",
    "    enable_caching=False,\n",
    "    parameter_values=csv_parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-2. BigQuery Preprocessing Pipeline Execution\n",
    "\n",
    "The BigQuery Criteo data preprocessing pipeline performs the following steps.  \n",
    "\n",
    " 1. Read the data from BigQuery tables.\n",
    " 2. Export the BigQuery data to Cloud Storage as Parquet files.\n",
    " 3. Fit a pre-defined NVTabular workflow to the training data split to calculate transformation statistics.\n",
    " 4. Transform the training and validation data splits using the fitted workflow.\n",
    " 5. Output transformed parquet files to Cloud Storage.\n",
    "\n",
    "\n",
    "<img src=\"./images/preprocessing_pipeline_bq.png\" alt=\"Pipeline\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting BigQuery data to Cloud Storage as Parquet files.\n",
    "\n",
    "In order to use the NVTabular to preprocess the BigQuery data, the data must be exported to Cloud Storage as Parquet files.\n",
    "The `extract_table_from_bq` method in the [src/preprocessing/etl.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/components.py) module exports the data from a BigQuery table to Cloud Storage as Paquet files. It uses the [extract_table](https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.client.Client.html#google.cloud.bigquery.client.Client.extract_table) API in the [BigQuery Python SDK](https://cloud.google.com/bigquery/docs/reference/libraries#client-libraries-install-python). The pipeline step is defined in the `export_parquet_from_bq_op` component in [src/pipelines/components.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/components.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pipeline configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BQ_DATASET_NAME = 'criteo_pipeline' # Set to your BigQuery dataset including the Criteo dataset.\n",
    "BQ_LOCATION = 'us' # Set to your BigQuery dataset location.\n",
    "BQ_TRAIN_TABLE_NAME = 'train'\n",
    "BQ_VALID_TABLE_NAME = 'valid'\n",
    "\n",
    "os.environ['PREPROCESS_BQ_PIPELINE_NAME'] = PREPROCESS_BQ_PIPELINE_NAME\n",
    "os.environ['PREPROCESS_BQ_PIPELINE_ROOT'] = PREPROCESS_BQ_PIPELINE_ROOT\n",
    "\n",
    "os.environ['BQ_DATASET_NAME'] = BQ_DATASET_NAME\n",
    "os.environ['BQ_LOCATION'] = BQ_LOCATION\n",
    "os.environ['BQ_TRAIN_TABLE_NAME'] = BQ_TRAIN_TABLE_NAME\n",
    "os.environ['BQ_VALID_TABLE_NAME'] = BQ_VALID_TABLE_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_parameter_values = {\n",
    "    'shuffle': json.dumps(None) # select PER_PARTITION, PER_WORKER, FULL, or None.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.preprocessing_pipelines import preprocessing_bq\n",
    "\n",
    "bq_compiled_pipeline_path = f'{PREPROCESS_BQ_PIPELINE_NAME}.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_bq,\n",
    "       package_path=bq_compiled_pipeline_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = f'{datetime.now().strftime(\"%Y%m%d%H%M%S\")}_{PREPROCESS_BQ_PIPELINE_NAME}'\n",
    "\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    display_name=job_name,\n",
    "    template_path=bq_compiled_pipeline_path,\n",
    "    enable_caching=False,\n",
    "    parameter_values=bq_parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "After completing this notebook you can proceed to the [02-model-training-hugectr.ipynb](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/02-model-training-hugectr.ipynb) notebook that demonstrates how to train DeepFM model using NVIDIA HugeCTR and Vertex AI."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m86",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m86"
  },
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
