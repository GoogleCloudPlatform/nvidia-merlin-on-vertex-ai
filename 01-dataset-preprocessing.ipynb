{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data Preprocessing at Scale with NVIDIA Merlin NVTabular and Vertex AI\n",
    "\n",
    "This notebook demonstrates how to preprocess data using [NVIDIA Merlin NVTabular](https://developer.nvidia.com/nvidia-merlin/nvtabular) and [Vertex AI](https://cloud.google.com/vertex-ai). The notebook covers the following:  \n",
    "1. NVTabular Overview.  \n",
    "2. Preprocessing Criteo Dataset.  \n",
    "3. Preprocessing Pipeline on Vertex AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Merlin NVTabular Overview\n",
    "\n",
    "Merlin NVTabular is a feature engineering and preprocessing library designed to effectively manipulate \n",
    "large datasets and significantly reduce data preparation time. The [core features](https://github.com/NVIDIA-Merlin/NVTabular/blob/main/docs/source/core_features.md) of NVTabular include:\n",
    "\n",
    "* Processes large datasets not bound by CPU or GPU memory.\n",
    "* Accelerates data preprocessing computation on NVIDIA GPUs using the [RAPIDS cuDF](https://github.com/rapidsai/cudf/tree/main/python/dask_cudf) library.\n",
    "* Supports multi-GPU and multi-node scaling with [DASK-CUDA](https://github.com/rapidsai/dask-cuda) and [dask.distributed](https://distributed.dask.org/en/latest/) parallelism.\n",
    "* Supports tabular data formats, including comma-separated values (CSV) files, Apache Parquet, Apache Orc, and Apache Avro.\n",
    "* Provides data loaders that are optimized for TensorFlow, PyTorch, and Merlin HugeCTR.\n",
    "* Includes multi-hot categoricals and vector continuous passing support to ease feature engineering.\n",
    "\n",
    "\n",
    "To preprocess the data, we need to define a transformation [`Workflow`](https://nvidia-merlin.github.io/NVTabular/main/api/workflow/workflow.html).  \n",
    "Each transformation step in the transformation pipeline executes multiple calculations, called `ops`. \n",
    "NVTabular provides a [set of ops](c), which include:\n",
    "\n",
    " - Filtering outliers or missing values, or creating new features indicating that a value is missing;\n",
    " - Imputing and filling in missing data;\n",
    " - Discretization or bucketing of continuous features;\n",
    " - Creating features by splitting or combining existing features, for example, breaking down a date column into day-of-week, month-of-year, day-of-month features;\n",
    " - Normalizing numerical features to have zero mean and unit variance or applying transformations, for example with log transform;\n",
    " - Encoding discrete features using one-hot vectors or converting them to continuous integer indices.  \n",
    "\n",
    "NVTabular processes a dataset, given a pre-defined workflow, in two steps:\n",
    "\n",
    "1. The `fit` step, where NVTabular compute the statistics required for transforming the data. Such a step requires at most `N` passes through the data, where `N` is the number of chained operations in the workflow.\n",
    "2. The `apply` step, where NVTabular uses the fitted workflow to process the data. \n",
    "\n",
    "NVTabular is designed to minimize the number of passes through the data. This is achieved with a lazy execution strategy. Data operations are not executed until an explicit apply phase. This allows NVTabular to optimize the workflow that requires iteration over the entire dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. Preprocessing Criteo dataset\n",
    "\n",
    "The Criteo dataset contains over four billion samples spanning 24 CSV files. Each record contains 40 columns: 13 columns are numerical, 26 columns are categorical, and 1 binary target column.  \n",
    "See [00-dataset-management.ipynb](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/00-dataset-management.ipynb) for more details.\n",
    "\n",
    "\n",
    "### NVTabular preprocessing Workflow for Criteo dataset\n",
    "\n",
    "In this example, the preprocessing `nvt.Workflow` consists for the following operations:\n",
    " - [Categorify](https://nvidia-merlin.github.io/NVTabular/main/api/ops/categorify.html): applied to categorical columns (column names that start with C). \n",
    " - [FillMissing](https://nvidia-merlin.github.io/NVTabular/main/api/ops/fillmissing.html): applied to continuous columns (column names that start with I).\n",
    " - [Clip](https://nvidia-merlin.github.io/NVTabular/main/api/ops/clip.html):  applied to continuous columns after FillMissing.\n",
    " - [Normalize](https://nvidia-merlin.github.io/NVTabular/main/api/ops/normalize.html): applied to continuous columns after Clip.\n",
    " \n",
    "<img src=\"images/dag_preprocessing.png\" alt=\"Pipeline\" style=\"width:30%;\"/>\n",
    " \n",
    " The `nvt.Workflow` is created in the `create_criteo_nvt_workflow` method, which can be found in [src/preprocessing/task.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/preprocessing/task.py) module.  \n",
    " This `nvt.Workflow` will be used as a guide to calculate the necessary statistics, and execute the data transformation.  \n",
    " \n",
    "\n",
    "### Implementing the preprocessing pipelines using KFP\n",
    "\n",
    "[src/pipelines/preprocessing_pipelines.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/preprocessing_pipelines.py) defines the KFP pipelines to preprocess the Criteo data. \n",
    "The `preprocessing_csv` processes the CSV data files in Cloud Storage.\n",
    "\n",
    "A pipeline component is a self-contained set of code that performs one step in your ML workflow. The pipeline uses the following components defined in [src/pipelines/components.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/components.py):\n",
    "\n",
    "1. `convert_csv_to_parquet_op`: this component converts raw CSV files to Parquet files, and store them to Cloud Storage. \n",
    "2. `analyze_dataset_op`: this component creates a Criteo preprocessing `nvt.Workflow`, fit it to the training data split, and store it to Cloud Storage.\n",
    "3. `transform_dataset_op`: this component loads the fitted `nvt.Workflow` from Cloud Storage, uses it to transform and input datas split, and store the transformed data as Parquet files to Cloud Storage.\n",
    "\n",
    "Each component is annotated with Inputs and Outputs to keep track of lineage metadata.  \n",
    "The docker image used to execute the components is defined in [Dockerfile.nvtabular](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/Dockerfile.nvtabular).  \n",
    "\n",
    "Some steps in the pipeline are configured to submit a custom Vertex AI Training job with the required CPU, memory and GPU configurations.  \n",
    "You can customize the pipeline by setting the variables in the [config.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/config.py) module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "In this section of the notebook you configure your environment settings, including a GCP project, a GCP compute region, a Vertex AI service account and a Vertex AI staging bucket. You also set the locations of training and validation splits in GCS.\n",
    "\n",
    "Make sure to update the below cells with the values reflecting your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from kfp.v2 import compiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project definitions\n",
    "PROJECT_ID = 'jk-mlops-dev' # Change to your project ID.\n",
    "REGION = 'us-central1' # Change to your region.\n",
    "\n",
    "# Bucket definitions\n",
    "BUCKET =  'jk-staging-us-central1' # Change to your bucket. All the files will be stored here.\n",
    "VERSION = 'v03'\n",
    "MODEL_DISPLAY_NAME = f'criteo-merlin-recommender-{VERSION}'\n",
    "WORKSPACE = f'gs://{BUCKET}/{MODEL_DISPLAY_NAME}'\n",
    "\n",
    "# Docker definitions\n",
    "IMAGE_NAME = 'nvt_preprocessing_test'\n",
    "IMAGE_URI = f'gcr.io/{PROJECT_ID}/{IMAGE_NAME}'\n",
    "DOCKERNAME = 'nvtabular'\n",
    "\n",
    "# Pipeline definitions\n",
    "PREPROCESS_CSV_PIPELINE_NAME = 'nvt-csv-pipeline'\n",
    "PREPROCESS_CSV_PIPELINE_ROOT = os.path.join(WORKSPACE, PREPROCESS_CSV_PIPELINE_NAME)\n",
    "\n",
    "# Instance configuration\n",
    "# Change if you need a different instance configuration\n",
    "GPU_LIMIT = '2'\n",
    "GPU_TYPE = 'NVIDIA_TESLA_A100'\n",
    "CPU_LIMIT = '24'\n",
    "MEMORY_LIMIT = '170'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set pipeline configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PROJECT_ID'] = PROJECT_ID\n",
    "os.environ['REGION'] = REGION\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['WORKSPACE'] = WORKSPACE\n",
    "\n",
    "os.environ['NVT_IMAGE_URI'] = IMAGE_URI\n",
    "os.environ['PREPROCESS_CSV_PIPELINE_NAME'] = PREPROCESS_CSV_PIPELINE_NAME\n",
    "os.environ['PREPROCESS_CSV_PIPELINE_ROOT'] = PREPROCESS_CSV_PIPELINE_ROOT\n",
    "os.environ['DOCKERNAME'] = DOCKERNAME\n",
    "\n",
    "os.environ['INSTANCE_TYPE'] = INSTANCE_TYPE\n",
    "os.environ['GPU_LIMIT'] = GPU_LIMIT\n",
    "os.environ['GPU_TYPE'] = GPU_TYPE\n",
    "os.environ['CPU_LIMIT'] = CPU_LIMIT\n",
    "os.environ['MEMORY_LIMIT'] = MEMORY_LIMIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Vertex SDK client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Vertex AI API\n",
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=os.path.join(WORKSPACE, 'stg') \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Container Docker Image\n",
    "\n",
    "The following command will build the Docker container image to the NVTabular preprocessing steps of the pipeline and push it to the [Google Container Registry](https://cloud.google.com/container-registry). \n",
    "\n",
    "Note that building the Docker container image take up to 20 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating temporary tarball archive of 51 file(s) totalling 5.0 MiB before compression.\n",
      "Some files were not included in the source upload.\n",
      "\n",
      "Check the gcloud log [/home/jupyter/.config/gcloud/logs/2022.02.27/18.04.48.317153.log] to see which files and the contents of the\n",
      "default gcloudignore file used (see `$ gcloud topic gcloudignore` to learn\n",
      "more).\n",
      "\n",
      "Uploading tarball of [.] to [gs://jk-mlops-dev_cloudbuild/source/1645985088.676192-d56de4ccb6974facbb959ddd748e4df6.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/jk-mlops-dev/locations/global/builds/e1b2c215-0846-48bf-a74f-d4dd54d12de5].\n",
      "Logs are available at [https://console.cloud.google.com/cloud-build/builds/e1b2c215-0846-48bf-a74f-d4dd54d12de5?project=895222332033].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"e1b2c215-0846-48bf-a74f-d4dd54d12de5\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://jk-mlops-dev_cloudbuild/source/1645985088.676192-d56de4ccb6974facbb959ddd748e4df6.tgz#1645985089663034\n",
      "Copying gs://jk-mlops-dev_cloudbuild/source/1645985088.676192-d56de4ccb6974facbb959ddd748e4df6.tgz#1645985089663034...\n",
      "/ [1 files][  4.3 MiB/  4.3 MiB]                                                \n",
      "Operation completed over 1 objects/4.3 MiB.\n",
      "BUILD\n",
      "Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Sending build context to Docker daemon  98.82kB\n",
      "Step 1/8 : FROM nvcr.io/nvidia/merlin/merlin-training:22.02\n",
      "22.02: Pulling from nvidia/merlin/merlin-training\n",
      "ea362f368469: Pulling fs layer\n",
      "946996a35715: Pulling fs layer\n",
      "b68628bdd658: Pulling fs layer\n",
      "067c279d0ed3: Pulling fs layer\n",
      "788ed670ac9a: Pulling fs layer\n",
      "5bce02e2160d: Pulling fs layer\n",
      "dde9cc1db7fe: Pulling fs layer\n",
      "db5cfd65155a: Pulling fs layer\n",
      "488189cee8db: Pulling fs layer\n",
      "8b6a34345376: Pulling fs layer\n",
      "f860e6a3e66f: Pulling fs layer\n",
      "1f89580a8279: Pulling fs layer\n",
      "9b4ca11389c4: Pulling fs layer\n",
      "9f82759f44ea: Pulling fs layer\n",
      "3e377ca26037: Pulling fs layer\n",
      "417871d308b5: Pulling fs layer\n",
      "2b704123c743: Pulling fs layer\n",
      "15a644f84e51: Pulling fs layer\n",
      "ec940adcf2ee: Pulling fs layer\n",
      "e890387af6bf: Pulling fs layer\n",
      "cfaf3d7b54d9: Pulling fs layer\n",
      "b9895e8a6996: Pulling fs layer\n",
      "a4b4243b0bad: Pulling fs layer\n",
      "361eaee35c2e: Pulling fs layer\n",
      "bb2f8d574349: Pulling fs layer\n",
      "d3dfe5a56976: Pulling fs layer\n",
      "f1f3b59d44f0: Pulling fs layer\n",
      "04f6cddb3c06: Pulling fs layer\n",
      "c9ccf247f0f1: Pulling fs layer\n",
      "537326ce6b59: Pulling fs layer\n",
      "ed08d0b1e1db: Pulling fs layer\n",
      "a43f0a2ec2bb: Pulling fs layer\n",
      "0c72ad5158a9: Pulling fs layer\n",
      "744508d6718f: Pulling fs layer\n",
      "31b56b672639: Pulling fs layer\n",
      "1885d1fa160b: Pulling fs layer\n",
      "caba4bd7b8a8: Pulling fs layer\n",
      "de32f78a8baf: Pulling fs layer\n",
      "a587ba3b82a5: Pulling fs layer\n",
      "1faded3bbec6: Pulling fs layer\n",
      "eaf7abae665d: Pulling fs layer\n",
      "05933d002591: Pulling fs layer\n",
      "16786525d0ff: Pulling fs layer\n",
      "fe76d2df4dab: Pulling fs layer\n",
      "e0212d2a735f: Pulling fs layer\n",
      "9cb226a8e9c7: Pulling fs layer\n",
      "b9e214ca9675: Pulling fs layer\n",
      "943de02b4016: Pulling fs layer\n",
      "6c572c017a75: Pulling fs layer\n",
      "78d18de1fb77: Pulling fs layer\n",
      "9b21a689868c: Pulling fs layer\n",
      "cdbea943ed88: Pulling fs layer\n",
      "1e4db4833b18: Pulling fs layer\n",
      "cd841a4e4587: Pulling fs layer\n",
      "067c279d0ed3: Waiting\n",
      "23145b8abab5: Pulling fs layer\n",
      "2202434f63b9: Pulling fs layer\n",
      "788ed670ac9a: Waiting\n",
      "62d2e539fb51: Pulling fs layer\n",
      "6bc0fef10ace: Pulling fs layer\n",
      "559153aa911f: Pulling fs layer\n",
      "5bce02e2160d: Waiting\n",
      "41134d5e7d44: Pulling fs layer\n",
      "dde9cc1db7fe: Waiting\n",
      "45be3cd66333: Pulling fs layer\n",
      "f91d546d1de7: Pulling fs layer\n",
      "f860e6a3e66f: Waiting\n",
      "bacc40e6ea2c: Pulling fs layer\n",
      "3e7bac56b528: Pulling fs layer\n",
      "b9d63dbdec59: Pulling fs layer\n",
      "8a833e989e22: Pulling fs layer\n",
      "878bc16faa36: Pulling fs layer\n",
      "3f035d3a609d: Pulling fs layer\n",
      "fb5c84b12616: Pulling fs layer\n",
      "db87536ab09d: Pulling fs layer\n",
      "1885d1fa160b: Waiting\n",
      "1e4db4833b18: Waiting\n",
      "cd841a4e4587: Waiting\n",
      "488189cee8db: Waiting\n",
      "de32f78a8baf: Waiting\n",
      "db5cfd65155a: Waiting\n",
      "1f89580a8279: Waiting\n",
      "caba4bd7b8a8: Waiting\n",
      "bb2f8d574349: Waiting\n",
      "e0212d2a735f: Waiting\n",
      "23145b8abab5: Waiting\n",
      "9cb226a8e9c7: Waiting\n",
      "2202434f63b9: Waiting\n",
      "8b6a34345376: Waiting\n",
      "62d2e539fb51: Waiting\n",
      "361eaee35c2e: Waiting\n",
      "9b4ca11389c4: Waiting\n",
      "15a644f84e51: Waiting\n",
      "b9e214ca9675: Waiting\n",
      "943de02b4016: Waiting\n",
      "3e377ca26037: Waiting\n",
      "6bc0fef10ace: Waiting\n",
      "9f82759f44ea: Waiting\n",
      "6c572c017a75: Waiting\n",
      "559153aa911f: Waiting\n",
      "417871d308b5: Waiting\n",
      "2b704123c743: Waiting\n",
      "78d18de1fb77: Waiting\n",
      "cdbea943ed88: Waiting\n",
      "ec940adcf2ee: Waiting\n",
      "e890387af6bf: Waiting\n",
      "9b21a689868c: Waiting\n",
      "cfaf3d7b54d9: Waiting\n",
      "45be3cd66333: Waiting\n",
      "3e7bac56b528: Waiting\n",
      "b9895e8a6996: Waiting\n",
      "f91d546d1de7: Waiting\n",
      "bacc40e6ea2c: Waiting\n",
      "a43f0a2ec2bb: Waiting\n",
      "05933d002591: Waiting\n",
      "16786525d0ff: Waiting\n",
      "878bc16faa36: Waiting\n",
      "f1f3b59d44f0: Waiting\n",
      "04f6cddb3c06: Waiting\n",
      "3f035d3a609d: Waiting\n",
      "fb5c84b12616: Waiting\n",
      "a4b4243b0bad: Waiting\n",
      "0c72ad5158a9: Waiting\n",
      "db87536ab09d: Waiting\n",
      "c9ccf247f0f1: Waiting\n",
      "537326ce6b59: Waiting\n",
      "744508d6718f: Waiting\n",
      "ed08d0b1e1db: Waiting\n",
      "eaf7abae665d: Waiting\n",
      "8a833e989e22: Waiting\n",
      "31b56b672639: Waiting\n",
      "fe76d2df4dab: Waiting\n",
      "ea362f368469: Verifying Checksum\n",
      "ea362f368469: Download complete\n",
      "067c279d0ed3: Download complete\n",
      "946996a35715: Verifying Checksum\n",
      "946996a35715: Download complete\n",
      "b68628bdd658: Verifying Checksum\n",
      "b68628bdd658: Download complete\n",
      "5bce02e2160d: Download complete\n",
      "dde9cc1db7fe: Verifying Checksum\n",
      "dde9cc1db7fe: Download complete\n",
      "db5cfd65155a: Verifying Checksum\n",
      "db5cfd65155a: Download complete\n",
      "488189cee8db: Verifying Checksum\n",
      "488189cee8db: Download complete\n",
      "ea362f368469: Pull complete\n",
      "8b6a34345376: Verifying Checksum\n",
      "8b6a34345376: Download complete\n",
      "1f89580a8279: Verifying Checksum\n",
      "1f89580a8279: Download complete\n",
      "f860e6a3e66f: Verifying Checksum\n",
      "f860e6a3e66f: Download complete\n",
      "9f82759f44ea: Verifying Checksum\n",
      "9f82759f44ea: Download complete\n",
      "3e377ca26037: Verifying Checksum\n",
      "3e377ca26037: Download complete\n",
      "417871d308b5: Verifying Checksum\n",
      "417871d308b5: Download complete\n",
      "2b704123c743: Verifying Checksum\n",
      "2b704123c743: Download complete\n",
      "9b4ca11389c4: Download complete\n",
      "ec940adcf2ee: Verifying Checksum\n",
      "ec940adcf2ee: Download complete\n",
      "15a644f84e51: Verifying Checksum\n",
      "15a644f84e51: Download complete\n",
      "946996a35715: Pull complete\n",
      "cfaf3d7b54d9: Verifying Checksum\n",
      "cfaf3d7b54d9: Download complete\n",
      "b9895e8a6996: Verifying Checksum\n",
      "b9895e8a6996: Download complete\n",
      "a4b4243b0bad: Verifying Checksum\n",
      "a4b4243b0bad: Download complete\n",
      "361eaee35c2e: Verifying Checksum\n",
      "361eaee35c2e: Download complete\n",
      "b68628bdd658: Pull complete\n",
      "067c279d0ed3: Pull complete\n",
      "e890387af6bf: Verifying Checksum\n",
      "e890387af6bf: Download complete\n",
      "d3dfe5a56976: Verifying Checksum\n",
      "d3dfe5a56976: Download complete\n",
      "f1f3b59d44f0: Download complete\n",
      "04f6cddb3c06: Verifying Checksum\n",
      "04f6cddb3c06: Download complete\n",
      "bb2f8d574349: Verifying Checksum\n",
      "bb2f8d574349: Download complete\n",
      "537326ce6b59: Verifying Checksum\n",
      "537326ce6b59: Download complete\n",
      "ed08d0b1e1db: Verifying Checksum\n",
      "ed08d0b1e1db: Download complete\n",
      "a43f0a2ec2bb: Verifying Checksum\n",
      "a43f0a2ec2bb: Download complete\n",
      "0c72ad5158a9: Download complete\n",
      "744508d6718f: Verifying Checksum\n",
      "744508d6718f: Download complete\n",
      "31b56b672639: Verifying Checksum\n",
      "31b56b672639: Download complete\n",
      "1885d1fa160b: Verifying Checksum\n",
      "1885d1fa160b: Download complete\n",
      "caba4bd7b8a8: Download complete\n",
      "de32f78a8baf: Verifying Checksum\n",
      "de32f78a8baf: Download complete\n",
      "a587ba3b82a5: Download complete\n",
      "1faded3bbec6: Download complete\n",
      "eaf7abae665d: Download complete\n",
      "05933d002591: Download complete\n",
      "16786525d0ff: Verifying Checksum\n",
      "16786525d0ff: Download complete\n",
      "788ed670ac9a: Verifying Checksum\n",
      "788ed670ac9a: Download complete\n",
      "e0212d2a735f: Download complete\n",
      "fe76d2df4dab: Verifying Checksum\n",
      "fe76d2df4dab: Download complete\n",
      "b9e214ca9675: Verifying Checksum\n",
      "b9e214ca9675: Download complete\n",
      "9cb226a8e9c7: Verifying Checksum\n",
      "9cb226a8e9c7: Download complete\n",
      "943de02b4016: Verifying Checksum\n",
      "943de02b4016: Download complete\n",
      "78d18de1fb77: Download complete\n",
      "6c572c017a75: Verifying Checksum\n",
      "6c572c017a75: Download complete\n",
      "cdbea943ed88: Verifying Checksum\n",
      "cdbea943ed88: Download complete\n",
      "1e4db4833b18: Verifying Checksum\n",
      "1e4db4833b18: Download complete\n",
      "9b21a689868c: Verifying Checksum\n",
      "9b21a689868c: Download complete\n",
      "cd841a4e4587: Verifying Checksum\n",
      "cd841a4e4587: Download complete\n",
      "23145b8abab5: Verifying Checksum\n",
      "23145b8abab5: Download complete\n",
      "62d2e539fb51: Verifying Checksum\n",
      "62d2e539fb51: Download complete\n",
      "2202434f63b9: Verifying Checksum\n",
      "2202434f63b9: Download complete\n",
      "6bc0fef10ace: Verifying Checksum\n",
      "6bc0fef10ace: Download complete\n",
      "559153aa911f: Verifying Checksum\n",
      "559153aa911f: Download complete\n",
      "41134d5e7d44: Verifying Checksum\n",
      "41134d5e7d44: Download complete\n",
      "f91d546d1de7: Verifying Checksum\n",
      "f91d546d1de7: Download complete\n",
      "c9ccf247f0f1: Verifying Checksum\n",
      "c9ccf247f0f1: Download complete\n",
      "45be3cd66333: Verifying Checksum\n",
      "45be3cd66333: Download complete\n",
      "b9d63dbdec59: Download complete\n",
      "8a833e989e22: Verifying Checksum\n",
      "8a833e989e22: Download complete\n",
      "878bc16faa36: Download complete\n",
      "3f035d3a609d: Verifying Checksum\n",
      "3f035d3a609d: Download complete\n",
      "bacc40e6ea2c: Verifying Checksum\n",
      "bacc40e6ea2c: Download complete\n",
      "db87536ab09d: Verifying Checksum\n",
      "db87536ab09d: Download complete\n",
      "fb5c84b12616: Verifying Checksum\n",
      "fb5c84b12616: Download complete\n",
      "3e7bac56b528: Verifying Checksum\n",
      "3e7bac56b528: Download complete\n",
      "788ed670ac9a: Pull complete\n",
      "5bce02e2160d: Pull complete\n",
      "dde9cc1db7fe: Pull complete\n",
      "db5cfd65155a: Pull complete\n",
      "488189cee8db: Pull complete\n",
      "8b6a34345376: Pull complete\n",
      "f860e6a3e66f: Pull complete\n",
      "1f89580a8279: Pull complete\n",
      "9b4ca11389c4: Pull complete\n",
      "9f82759f44ea: Pull complete\n",
      "3e377ca26037: Pull complete\n",
      "417871d308b5: Pull complete\n",
      "2b704123c743: Pull complete\n",
      "15a644f84e51: Pull complete\n",
      "ec940adcf2ee: Pull complete\n",
      "e890387af6bf: Pull complete\n",
      "cfaf3d7b54d9: Pull complete\n",
      "b9895e8a6996: Pull complete\n",
      "a4b4243b0bad: Pull complete\n",
      "361eaee35c2e: Pull complete\n",
      "bb2f8d574349: Pull complete\n",
      "d3dfe5a56976: Pull complete\n",
      "f1f3b59d44f0: Pull complete\n",
      "04f6cddb3c06: Pull complete\n",
      "c9ccf247f0f1: Pull complete\n",
      "537326ce6b59: Pull complete\n",
      "ed08d0b1e1db: Pull complete\n",
      "a43f0a2ec2bb: Pull complete\n",
      "0c72ad5158a9: Pull complete\n",
      "744508d6718f: Pull complete\n",
      "31b56b672639: Pull complete\n",
      "1885d1fa160b: Pull complete\n",
      "caba4bd7b8a8: Pull complete\n",
      "de32f78a8baf: Pull complete\n",
      "a587ba3b82a5: Pull complete\n",
      "1faded3bbec6: Pull complete\n",
      "eaf7abae665d: Pull complete\n",
      "05933d002591: Pull complete\n",
      "16786525d0ff: Pull complete\n",
      "fe76d2df4dab: Pull complete\n",
      "e0212d2a735f: Pull complete\n",
      "9cb226a8e9c7: Pull complete\n",
      "b9e214ca9675: Pull complete\n",
      "943de02b4016: Pull complete\n",
      "6c572c017a75: Pull complete\n",
      "78d18de1fb77: Pull complete\n",
      "9b21a689868c: Pull complete\n",
      "cdbea943ed88: Pull complete\n",
      "1e4db4833b18: Pull complete\n",
      "cd841a4e4587: Pull complete\n",
      "23145b8abab5: Pull complete\n",
      "2202434f63b9: Pull complete\n",
      "62d2e539fb51: Pull complete\n",
      "6bc0fef10ace: Pull complete\n",
      "559153aa911f: Pull complete\n",
      "41134d5e7d44: Pull complete\n",
      "45be3cd66333: Pull complete\n",
      "f91d546d1de7: Pull complete\n",
      "bacc40e6ea2c: Pull complete\n",
      "3e7bac56b528: Pull complete\n",
      "b9d63dbdec59: Pull complete\n",
      "8a833e989e22: Pull complete\n",
      "878bc16faa36: Pull complete\n",
      "3f035d3a609d: Pull complete\n",
      "fb5c84b12616: Pull complete\n",
      "db87536ab09d: Pull complete\n",
      "Digest: sha256:8b3591d7e9648c2e7806fbeadc48ff0c761e704acb1d1187a92def9e375efbbf\n",
      "Status: Downloaded newer image for nvcr.io/nvidia/merlin/merlin-training:22.02\n",
      " ---> 8ab584249ef2\n",
      "Step 2/8 : WORKDIR /src\n",
      " ---> Running in a57135689368\n",
      "Removing intermediate container a57135689368\n",
      " ---> 07adc097aa86\n",
      "Step 3/8 : RUN pip install -U pip\n",
      " ---> Running in 426b5166f60d\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: pip in /usr/local/lib/python3.8/dist-packages (21.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-22.0.3-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.3.1\n",
      "    Uninstalling pip-21.3.1:\n",
      "      Successfully uninstalled pip-21.3.1\n",
      "Successfully installed pip-22.0.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 426b5166f60d\n",
      " ---> 5c8e5f5c23c4\n",
      "Step 4/8 : RUN pip install google-cloud-bigquery gcsfs\n",
      " ---> Running in 23b47fcc0873\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-2.34.0-py2.py3-none-any.whl (206 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 206.1/206.1 KB 14.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: gcsfs in /usr/local/lib/python3.8/dist-packages (2022.1.0)\n",
      "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (3.19.4)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.38.1 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (1.41.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (2.27.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (2.1.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (21.3)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.29.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery) (2.2.2)\n",
      "Collecting proto-plus>=1.10.0\n",
      "  Downloading proto_plus-1.20.3-py3-none-any.whl (46 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.2/46.2 KB 149.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec==2022.01.0 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (2022.1.0)\n",
      "Requirement already satisfied: google-auth-oauthlib in /usr/local/lib/python3.8/dist-packages (from gcsfs) (0.4.6)\n",
      "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.8/dist-packages (from gcsfs) (2.1.0)\n",
      "Requirement already satisfied: aiohttp<4 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (3.8.1)\n",
      "Requirement already satisfied: decorator>4.1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (5.1.1)\n",
      "Requirement already satisfied: google-auth>=1.2 in /usr/local/lib/python3.8/dist-packages (from gcsfs) (1.35.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4->gcsfs) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4->gcsfs) (1.7.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4->gcsfs) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4->gcsfs) (6.0.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4->gcsfs) (21.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4->gcsfs) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp<4->gcsfs) (2.0.10)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.29.0->google-cloud-bigquery) (1.54.0)\n",
      "Collecting grpcio-status<2.0dev,>=1.33.2\n",
      "  Downloading grpcio_status-1.44.0-py3-none-any.whl (10.0 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (1.15.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (0.2.8)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (59.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (4.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth>=1.2->gcsfs) (4.2.4)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=14.3->google-cloud-bigquery) (3.0.6)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-bigquery) (1.26.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib->gcsfs) (1.3.0)\n",
      "Collecting grpcio<2.0dev,>=1.38.1\n",
      "  Downloading grpcio-1.44.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.3/4.3 MB 149.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.2->gcsfs) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->gcsfs) (3.1.1)\n",
      "Installing collected packages: proto-plus, grpcio, grpcio-status, google-cloud-bigquery\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.41.0\n",
      "    Uninstalling grpcio-1.41.0:\n",
      "      Successfully uninstalled grpcio-1.41.0\n",
      "Successfully installed google-cloud-bigquery-2.34.0 grpcio-1.44.0 grpcio-status-1.44.0 proto-plus-1.20.3\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 23b47fcc0873\n",
      " ---> 7deae876d28b\n",
      "Step 5/8 : RUN pip install google-cloud-aiplatform kfp\n",
      " ---> Running in 81c6c30c4501\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.10.0-py2.py3-none-any.whl (1.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 50.5 MB/s eta 0:00:00\n",
      "Collecting kfp\n",
      "  Downloading kfp-1.8.11.tar.gz (298 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 298.6/298.6 KB 221.0 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: google-cloud-bigquery<3.0.0dev,>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform) (2.34.0)\n",
      "Collecting google-cloud-storage<2.0.0dev,>=1.32.0\n",
      "  Downloading google_cloud_storage-1.44.0-py2.py3-none-any.whl (106 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 106.8/106.8 KB 174.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform) (1.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform) (21.3)\n",
      "Requirement already satisfied: google-api-core[grpc]<3.0.0dev,>=1.26.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-aiplatform) (2.5.0)\n",
      "Requirement already satisfied: absl-py<2,>=0.9 in /usr/local/lib/python3.8/dist-packages (from kfp) (0.12.0)\n",
      "Collecting PyYAML<6,>=5.3\n",
      "  Downloading PyYAML-5.4.1-cp38-cp38-manylinux1_x86_64.whl (662 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 662.4/662.4 KB 218.0 MB/s eta 0:00:00\n",
      "Collecting kubernetes<19,>=8.0.0\n",
      "  Downloading kubernetes-18.20.0-py2.py3-none-any.whl (1.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 234.9 MB/s eta 0:00:00\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.10-py2.py3-none-any.whl (61 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.5/61.5 KB 164.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: google-auth<2,>=1.6.1 in /usr/local/lib/python3.8/dist-packages (from kfp) (1.35.0)\n",
      "Collecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.3/54.3 KB 147.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: cloudpickle<3,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from kfp) (2.0.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.8.1.tar.gz (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.2/54.2 KB 168.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting jsonschema<4,>=3.0.1\n",
      "  Downloading jsonschema-3.2.0-py2.py3-none-any.whl (56 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 KB 153.9 MB/s eta 0:00:00\n",
      "Collecting tabulate<1,>=0.8.6\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: click<9,>=7.1.2 in /usr/local/lib/python3.8/dist-packages (from kfp) (8.0.3)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.10.tar.gz (29 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.13.tar.gz (23 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting kfp-pipeline-spec<0.2.0,>=0.1.13\n",
      "  Downloading kfp_pipeline_spec-0.1.13-py3-none-any.whl (18 kB)\n",
      "Collecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.7/87.7 KB 182.3 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: protobuf<4,>=3.13.0 in /usr/local/lib/python3.8/dist-packages (from kfp) (3.19.4)\n",
      "Collecting uritemplate<4,>=3.0.1\n",
      "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
      "Collecting pydantic<2,>=1.8.2\n",
      "  Downloading pydantic-1.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.6/12.6 MB 202.2 MB/s eta 0:00:00\n",
      "Collecting typer<1.0,>=0.3.2\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: typing-extensions<4,>=3.7.4 in /usr/local/lib/python3.8/dist-packages (from kfp) (3.7.4.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from absl-py<2,>=0.9->kfp) (1.15.0)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.8/dist-packages (from Deprecated<2,>=1.2.7->kfp) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.8/dist-packages (from fire<1,>=0.3.1->kfp) (1.1.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.52.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.54.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.27.1)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.44.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /usr/local/lib/python3.8/dist-packages (from google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (1.44.0)\n",
      "Collecting google-auth-httplib2>=0.0.3\n",
      "  Downloading google_auth_httplib2-0.1.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Collecting httplib2<1dev,>=0.15.0\n",
      "  Downloading httplib2-0.20.4-py3-none-any.whl (96 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 96.6/96.6 KB 182.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.1->kfp) (59.4.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.1->kfp) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.1->kfp) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<2,>=1.6.1->kfp) (0.2.8)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.1.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.2.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /usr/local/lib/python3.8/dist-packages (from google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema<4,>=3.0.1->kfp) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema<4,>=3.0.1->kfp) (21.4.0)\n",
      "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.8/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (1.26.8)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp) (2021.10.8)\n",
      "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.8/dist-packages (from kubernetes<19,>=8.0.0->kfp) (1.3.0)\n",
      "Collecting websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0\n",
      "  Downloading websocket_client-1.3.1-py3-none-any.whl (54 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.2/54.2 KB 121.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=14.3->google-cloud-aiplatform) (3.0.6)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.8/dist-packages (from strip-hints<1,>=0.1.8->kfp) (0.37.1)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.8/dist-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<3.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.1->kfp) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (2.0.10)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]<3.0.0dev,>=1.26.0->google-cloud-aiplatform) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib->kubernetes<19,>=8.0.0->kfp) (3.1.1)\n",
      "Building wheels for collected packages: kfp, docstring-parser, fire, kfp-server-api, strip-hints\n",
      "  Building wheel for kfp (setup.py): started\n",
      "  Building wheel for kfp (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp: filename=kfp-1.8.11-py3-none-any.whl size=414450 sha256=6491dbd3e12a58aad8fb22c6cff4a6f3a8ac42d186fa5e55be527e29476be984\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xsxvuc26/wheels/4d/7d/13/d729de1d03c20cd8c4735516440d28b09aa964615e448100e5\n",
      "  Building wheel for docstring-parser (pyproject.toml): started\n",
      "  Building wheel for docstring-parser (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for docstring-parser: filename=docstring_parser-0.13-py3-none-any.whl size=31866 sha256=93b0e4c65523d4afea97688b39843c7b8e052ba41b6d361dbb911df79f95f83f\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xsxvuc26/wheels/26/52/35/0f8ba5e7897d1eb4a5276001de44a20a19e220eaf266f518d4\n",
      "  Building wheel for fire (setup.py): started\n",
      "  Building wheel for fire (setup.py): finished with status 'done'\n",
      "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=676f0ce64b09b9b49ac4ee785de2b7519698146b1fccd7b7092f44a0b6cf1ce8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xsxvuc26/wheels/1f/10/06/2a990ee4d73a8479fe2922445e8a876d38cfbfed052284c6a1\n",
      "  Building wheel for kfp-server-api (setup.py): started\n",
      "  Building wheel for kfp-server-api (setup.py): finished with status 'done'\n",
      "  Created wheel for kfp-server-api: filename=kfp_server_api-1.8.1-py3-none-any.whl size=95549 sha256=e13d17c8fe5cd1bb3651c2c13191d48afa56f8ca982a58ca6b52bd7cbfc4e16c\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xsxvuc26/wheels/00/8f/c8/711af4c86b4f64b69e06ac0547614ed08bbf3e802006340e97\n",
      "  Building wheel for strip-hints (setup.py): started\n",
      "  Building wheel for strip-hints (setup.py): finished with status 'done'\n",
      "  Created wheel for strip-hints: filename=strip_hints-0.1.10-py2.py3-none-any.whl size=22302 sha256=b3f8015ed12a60c9b1d617462646598aa3707f65fe1cafbfdcd69c6866517f33\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xsxvuc26/wheels/28/16/9b/7c21f4d08b98d02819658600b738d3453b2ffee3c9b757629e\n",
      "Successfully built kfp docstring-parser fire kfp-server-api strip-hints\n",
      "Installing collected packages: tabulate, websocket-client, uritemplate, typer, strip-hints, PyYAML, pydantic, kfp-pipeline-spec, jsonschema, httplib2, fire, docstring-parser, Deprecated, requests-toolbelt, kfp-server-api, kubernetes, google-auth-httplib2, google-api-python-client, google-cloud-storage, kfp, google-cloud-aiplatform\n",
      "  Attempting uninstall: PyYAML\n",
      "    Found existing installation: PyYAML 6.0\n",
      "    Uninstalling PyYAML-6.0:\n",
      "      Successfully uninstalled PyYAML-6.0\n",
      "  Attempting uninstall: jsonschema\n",
      "    Found existing installation: jsonschema 4.4.0\n",
      "    Uninstalling jsonschema-4.4.0:\n",
      "      Successfully uninstalled jsonschema-4.4.0\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.1.0\n",
      "    Uninstalling google-cloud-storage-2.1.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.1.0\n",
      "\u001b[91mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "nvtabular 0.10.0 requires numba>=0.55.1, but you have numba 0.53.1 which is incompatible.\n",
      "\u001b[0mSuccessfully installed Deprecated-1.2.13 PyYAML-5.4.1 docstring-parser-0.13 fire-0.4.0 google-api-python-client-1.12.10 google-auth-httplib2-0.1.0 google-cloud-aiplatform-1.10.0 google-cloud-storage-1.44.0 httplib2-0.20.4 jsonschema-3.2.0 kfp-1.8.11 kfp-pipeline-spec-0.1.13 kfp-server-api-1.8.1 kubernetes-18.20.0 pydantic-1.9.0 requests-toolbelt-0.9.1 strip-hints-0.1.10 tabulate-0.8.9 typer-0.4.0 uritemplate-3.0.1 websocket-client-1.3.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0mRemoving intermediate container 81c6c30c4501\n",
      " ---> 6606fd1d8fd4\n",
      "Step 6/8 : COPY preprocessing/* ./\n",
      " ---> 9afa128c2dee\n",
      "Step 7/8 : COPY serving/ serving/\n",
      " ---> a13919203986\n",
      "Step 8/8 : ENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda/compat/lib.real:/usr/local/hugectr/lib:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/cuda/compat/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64:/usr/local/lib:/repos/dist/lib\n",
      " ---> Running in fdded3401961\n",
      "Removing intermediate container fdded3401961\n",
      " ---> d31df727f3cd\n",
      "Successfully built d31df727f3cd\n",
      "Successfully tagged gcr.io/jk-mlops-dev/nvt_preprocessing_test:latest\n",
      "PUSH\n",
      "Pushing gcr.io/jk-mlops-dev/nvt_preprocessing_test\n",
      "The push refers to repository [gcr.io/jk-mlops-dev/nvt_preprocessing_test]\n",
      "f7d03b346892: Preparing\n",
      "63b2d407a055: Preparing\n",
      "1d28febeb44d: Preparing\n",
      "2a4c80e6db4c: Preparing\n",
      "e230f557f960: Preparing\n",
      "f87f639dbaa2: Preparing\n",
      "0533ff20d963: Preparing\n",
      "f9928355baa9: Preparing\n",
      "a032c0c5c15a: Preparing\n",
      "c50b26098398: Preparing\n",
      "52375419f35c: Preparing\n",
      "1dafd3a91e26: Preparing\n",
      "f0f0148ea791: Preparing\n",
      "489e73688a48: Preparing\n",
      "c5389f7ceaa5: Preparing\n",
      "8d10732914ee: Preparing\n",
      "c704bd2072a9: Preparing\n",
      "2b7eed8b1a12: Preparing\n",
      "eb1e819a23c6: Preparing\n",
      "5c74437d70ed: Preparing\n",
      "8d9d2aa56b09: Preparing\n",
      "428dce07c836: Preparing\n",
      "917561cd8b56: Preparing\n",
      "353d96006757: Preparing\n",
      "9c87fe94bffd: Preparing\n",
      "2277323cd799: Preparing\n",
      "bac5a913bca8: Preparing\n",
      "ffbc2a59c7c9: Preparing\n",
      "42423441deaf: Preparing\n",
      "6ebd5670fb1c: Preparing\n",
      "7e8eacf92de3: Preparing\n",
      "11aa67084b02: Preparing\n",
      "033ecf9aba2b: Preparing\n",
      "9c6f8a92da64: Preparing\n",
      "919d19f0ec7b: Preparing\n",
      "59956365e077: Preparing\n",
      "07e42acfe3db: Preparing\n",
      "5f41cb2ded09: Preparing\n",
      "6a8c3dc03546: Preparing\n",
      "e252786ed351: Preparing\n",
      "f33abcd6277c: Preparing\n",
      "695f42877b7b: Preparing\n",
      "e0ac5ecc5e42: Preparing\n",
      "6dbcbac96230: Preparing\n",
      "fd4c7055703f: Preparing\n",
      "45c73a96a886: Preparing\n",
      "21ab0479cc14: Preparing\n",
      "7c1645d546b1: Preparing\n",
      "58f7bad2791c: Preparing\n",
      "c86b9699e168: Preparing\n",
      "dd0d8f4e261e: Preparing\n",
      "cbeb7dbc6bca: Preparing\n",
      "902404deef79: Preparing\n",
      "1dda8d9ca658: Preparing\n",
      "ba8bbd1e5260: Preparing\n",
      "c975d4eab93e: Preparing\n",
      "ffbc2a59c7c9: Waiting\n",
      "be0d03c500ab: Preparing\n",
      "ce4b6a38594d: Preparing\n",
      "42423441deaf: Waiting\n",
      "8c0d53b05f26: Preparing\n",
      "5f6df865f6b8: Preparing\n",
      "8d69bfe06f25: Preparing\n",
      "6ebd5670fb1c: Waiting\n",
      "7e8eacf92de3: Waiting\n",
      "73491093ce01: Preparing\n",
      "f42a4d54ab6c: Preparing\n",
      "11aa67084b02: Waiting\n",
      "65fbdcb16d7f: Preparing\n",
      "3d1f6fef3119: Preparing\n",
      "033ecf9aba2b: Waiting\n",
      "b03019722fae: Preparing\n",
      "9c6f8a92da64: Waiting\n",
      "a4b8e0bb437c: Preparing\n",
      "96ebee343cb8: Preparing\n",
      "919d19f0ec7b: Waiting\n",
      "f7ff94bde60b: Preparing\n",
      "2dc8262b816c: Preparing\n",
      "6f8e6ee487f7: Preparing\n",
      "59956365e077: Waiting\n",
      "443046eeb9c3: Preparing\n",
      "07e42acfe3db: Waiting\n",
      "25e565306eb3: Preparing\n",
      "46481fe283a8: Preparing\n",
      "5f41cb2ded09: Waiting\n",
      "dafa6d186137: Preparing\n",
      "0eba131dffd0: Preparing\n",
      "f87f639dbaa2: Waiting\n",
      "6a8c3dc03546: Waiting\n",
      "0533ff20d963: Waiting\n",
      "f33abcd6277c: Waiting\n",
      "e252786ed351: Waiting\n",
      "695f42877b7b: Waiting\n",
      "f9928355baa9: Waiting\n",
      "2b7eed8b1a12: Waiting\n",
      "c5389f7ceaa5: Waiting\n",
      "eb1e819a23c6: Waiting\n",
      "5f6df865f6b8: Waiting\n",
      "5c74437d70ed: Waiting\n",
      "a032c0c5c15a: Waiting\n",
      "e0ac5ecc5e42: Waiting\n",
      "8d10732914ee: Waiting\n",
      "8d69bfe06f25: Waiting\n",
      "c704bd2072a9: Waiting\n",
      "6dbcbac96230: Waiting\n",
      "c50b26098398: Waiting\n",
      "52375419f35c: Waiting\n",
      "fd4c7055703f: Waiting\n",
      "2dc8262b816c: Waiting\n",
      "f42a4d54ab6c: Waiting\n",
      "1dafd3a91e26: Waiting\n",
      "45c73a96a886: Waiting\n",
      "f0f0148ea791: Waiting\n",
      "489e73688a48: Waiting\n",
      "6f8e6ee487f7: Waiting\n",
      "1dda8d9ca658: Waiting\n",
      "65fbdcb16d7f: Waiting\n",
      "443046eeb9c3: Waiting\n",
      "21ab0479cc14: Waiting\n",
      "ba8bbd1e5260: Waiting\n",
      "3d1f6fef3119: Waiting\n",
      "25e565306eb3: Waiting\n",
      "7c1645d546b1: Waiting\n",
      "c975d4eab93e: Waiting\n",
      "b03019722fae: Waiting\n",
      "46481fe283a8: Waiting\n",
      "58f7bad2791c: Waiting\n",
      "be0d03c500ab: Waiting\n",
      "a4b8e0bb437c: Waiting\n",
      "dafa6d186137: Waiting\n",
      "ce4b6a38594d: Waiting\n",
      "0eba131dffd0: Waiting\n",
      "8c0d53b05f26: Waiting\n",
      "f7ff94bde60b: Waiting\n",
      "96ebee343cb8: Waiting\n",
      "dd0d8f4e261e: Waiting\n",
      "8d9d2aa56b09: Waiting\n",
      "c86b9699e168: Waiting\n",
      "428dce07c836: Waiting\n",
      "353d96006757: Waiting\n",
      "cbeb7dbc6bca: Waiting\n",
      "2277323cd799: Waiting\n",
      "917561cd8b56: Waiting\n",
      "63b2d407a055: Pushed\n",
      "f7d03b346892: Pushed\n",
      "0533ff20d963: Layer already exists\n",
      "f9928355baa9: Layer already exists\n",
      "a032c0c5c15a: Layer already exists\n",
      "c50b26098398: Layer already exists\n",
      "52375419f35c: Layer already exists\n",
      "1dafd3a91e26: Layer already exists\n",
      "f0f0148ea791: Layer already exists\n",
      "489e73688a48: Layer already exists\n",
      "c5389f7ceaa5: Layer already exists\n",
      "e230f557f960: Pushed\n",
      "8d10732914ee: Layer already exists\n",
      "2a4c80e6db4c: Pushed\n",
      "c704bd2072a9: Layer already exists\n",
      "eb1e819a23c6: Layer already exists\n",
      "2b7eed8b1a12: Layer already exists\n",
      "5c74437d70ed: Layer already exists\n",
      "428dce07c836: Layer already exists\n",
      "8d9d2aa56b09: Layer already exists\n",
      "917561cd8b56: Layer already exists\n",
      "353d96006757: Layer already exists\n",
      "9c87fe94bffd: Layer already exists\n",
      "2277323cd799: Layer already exists\n",
      "bac5a913bca8: Layer already exists\n",
      "42423441deaf: Layer already exists\n",
      "ffbc2a59c7c9: Layer already exists\n",
      "6ebd5670fb1c: Layer already exists\n",
      "11aa67084b02: Layer already exists\n",
      "7e8eacf92de3: Layer already exists\n",
      "033ecf9aba2b: Layer already exists\n",
      "9c6f8a92da64: Layer already exists\n",
      "919d19f0ec7b: Layer already exists\n",
      "59956365e077: Layer already exists\n",
      "07e42acfe3db: Layer already exists\n",
      "5f41cb2ded09: Layer already exists\n",
      "6a8c3dc03546: Layer already exists\n",
      "e252786ed351: Layer already exists\n",
      "f33abcd6277c: Layer already exists\n",
      "695f42877b7b: Layer already exists\n",
      "e0ac5ecc5e42: Layer already exists\n",
      "f87f639dbaa2: Pushed\n",
      "6dbcbac96230: Layer already exists\n",
      "fd4c7055703f: Layer already exists\n",
      "21ab0479cc14: Layer already exists\n",
      "45c73a96a886: Layer already exists\n",
      "7c1645d546b1: Layer already exists\n",
      "58f7bad2791c: Layer already exists\n",
      "c86b9699e168: Layer already exists\n",
      "dd0d8f4e261e: Layer already exists\n",
      "cbeb7dbc6bca: Layer already exists\n",
      "902404deef79: Layer already exists\n",
      "1dda8d9ca658: Layer already exists\n",
      "ba8bbd1e5260: Layer already exists\n",
      "c975d4eab93e: Layer already exists\n",
      "be0d03c500ab: Layer already exists\n",
      "ce4b6a38594d: Layer already exists\n",
      "8c0d53b05f26: Layer already exists\n",
      "5f6df865f6b8: Layer already exists\n",
      "8d69bfe06f25: Layer already exists\n",
      "73491093ce01: Layer already exists\n",
      "65fbdcb16d7f: Layer already exists\n",
      "f42a4d54ab6c: Layer already exists\n",
      "3d1f6fef3119: Layer already exists\n",
      "b03019722fae: Layer already exists\n",
      "a4b8e0bb437c: Layer already exists\n",
      "96ebee343cb8: Layer already exists\n",
      "f7ff94bde60b: Layer already exists\n",
      "2dc8262b816c: Layer already exists\n",
      "6f8e6ee487f7: Layer already exists\n",
      "443046eeb9c3: Layer already exists\n",
      "25e565306eb3: Layer already exists\n",
      "46481fe283a8: Layer already exists\n",
      "0eba131dffd0: Layer already exists\n",
      "dafa6d186137: Layer already exists\n",
      "1d28febeb44d: Pushed\n",
      "latest: digest: sha256:7fbb6d4c832e011a575abe8b94f2f690562bd7c29e95cab4b236f6c777eb3195 size: 16291\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                      IMAGES                                                STATUS\n",
      "e1b2c215-0846-48bf-a74f-d4dd54d12de5  2022-02-27T18:04:49+00:00  4M10S     gs://jk-mlops-dev_cloudbuild/source/1645985088.676192-d56de4ccb6974facbb959ddd748e4df6.tgz  gcr.io/jk-mlops-dev/nvt_preprocessing_test (+1 more)  SUCCESS\n"
     ]
    }
   ],
   "source": [
    "FILE_LOCATION = './src'\n",
    "! gcloud builds submit --config src/cloudbuild.yaml --substitutions _DOCKERNAME=$DOCKERNAME,_IMAGE_URI=$IMAGE_URI,_FILE_LOCATION=$FILE_LOCATION --timeout=2h --machine-type=e2-highcpu-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-1. CSV Preprocessing Pipeline Execution\n",
    "\n",
    "The CSV Criteo data preprocessing pipeline performs the following steps.  \n",
    "\n",
    " 1. Read CSV files from Cloud Storage.\n",
    " 2. Convert the CSV files to parquet format and write it Cloud Storage.\n",
    " 3. Fit a pre-defined NVTabular workflow to the training data split to calculate transformation statistics.\n",
    " 4. Transform the training and validation data splits using the fitted workflow.\n",
    " 5. Output transformed parquet files to Cloud Storage.\n",
    "\n",
    "<img src=\"./images/preprocessing_pipeline_csv.png\" alt=\"Pipeline\" style=\"height: 50%; width:50%;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting CSV files to Parquet with NVTabular\n",
    "\n",
    "The Criteo dataset is provided in TSV format, but the recommended data format to run the NVTabular preprocessing task and get the best possible performance is [Parquet](http://parquet.apache.org/documentation/latest/); a compressed, column-oriented file structure format. While NVTabular also supports reading from CSV files, reading  \n",
    "Parquet files can be 2x faster than reading CSV files.  \n",
    "\n",
    "To convert the Criteo CSV data to Parquet, the following steps are performed:\n",
    "\n",
    "1. Create a `nvt.Dataset` object the CSV data using the `create_csv_dataset` method in [src/preprocessing/task.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/preprocessing/task.py).\n",
    "2. Convert the CSV data to Parquet, and write it to Cloud Storahe using the `convert_csv_to_parquet` method in [src/preprocessing/task.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/preprocessing/task.py).\n",
    "\n",
    "The pipeline uses the `convert_csv_to_parquet_op` component, which is implemented in [src/pipelines/components.py](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/src/pipelines/components.py) which submits a Vertex AI training job to convert the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create pipeline parameters\n",
    "\n",
    "NVTabular provides an option to shuffle the dataset before storing to disk.  \n",
    "The uniformly shuffled dataset enables the data loader to read in contiguous chunks of data that are already randomized across the entire dataset.\n",
    "NVTabular provides the option to control the number of chunks that are combined into a batch, allowing the end user flexibility when trading off between performance and true randomization.  \n",
    "This mechanism is critical when dealing with datasets that exceed CPU memory and per-epoch shuffling is desired during training.  \n",
    "Full shuffling of such a dataset can exceed training time for the epoch by several orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of path(s) to criteo file(s) or folder(s) in GCS.\n",
    "# Training files\n",
    "\n",
    "CRITEO_BASE_PATH = 'gs://jk-criteo-bucket/criteo_raw_tsv'\n",
    "\n",
    "TRAIN_PATHS = [\n",
    "    f'{CRITEO_BASE_PATH}/day_{i}' for i in range(23)\n",
    "]\n",
    "\n",
    "# Validation files\n",
    "VALID_PATHS = [f'{CRITEO_BASE_PATH}/day_23'] \n",
    "\n",
    "sep = '\\t' # Separator for the CSV file.\n",
    "num_output_files_train = len(TRAIN_PATHS) # Number of output files after converting CSV to Parquet\n",
    "num_output_files_valid = len(VALID_PATHS) # Number of output files after converting CSV to Parquet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_parameter_values = {\n",
    "    'train_paths': json.dumps(TRAIN_PATHS),\n",
    "    'valid_paths': json.dumps(VALID_PATHS),\n",
    "    'sep': sep,\n",
    "    'num_output_files_train': num_output_files_train,\n",
    "    'num_output_files_valid': num_output_files_valid,\n",
    "    'shuffle': json.dumps(None) # select PER_PARTITION, PER_WORKER, FULL, or None.\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile KFP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.pipelines.preprocessing_pipelines import preprocessing_csv\n",
    "\n",
    "csv_compiled_pipeline_path = f'{PREPROCESS_CSV_PIPELINE_NAME}.json'\n",
    "compiler.Compiler().compile(\n",
    "       pipeline_func=preprocessing_csv,\n",
    "       package_path=csv_compiled_pipeline_path\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit job to Vertex AI Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/895222332033/locations/us-central1/pipelineJobs/nvt-csv-pipeline-20220227180940\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/895222332033/locations/us-central1/pipelineJobs/nvt-csv-pipeline-20220227180940')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/nvt-csv-pipeline-20220227180940?project=895222332033\n"
     ]
    }
   ],
   "source": [
    "job_name = f'{datetime.now().strftime(\"%Y%m%d%H%M%S\")}_{PREPROCESS_CSV_PIPELINE_NAME}'\n",
    "\n",
    "pipeline_job = vertex_ai.PipelineJob(\n",
    "    display_name=job_name,\n",
    "    template_path=csv_compiled_pipeline_path,\n",
    "    enable_caching=False,\n",
    "    parameter_values=csv_parameter_values,\n",
    ")\n",
    "\n",
    "pipeline_job.submit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "After completing this notebook you can proceed to the [02-model-training-hugectr.ipynb](https://github.com/GoogleCloudPlatform/nvidia-merlin-on-vertex-ai/blob/main/02-model-training-hugectr.ipynb) notebook that demonstrates how to train DeepFM model using NVIDIA HugeCTR and Vertex AI."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m87",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m87"
  },
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
